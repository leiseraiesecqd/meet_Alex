{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import time\n",
    "import datetime\n",
    "import os,gc\n",
    "import utils\n",
    "\n",
    "file_path='D:/360WiFi/'\n",
    "raw_data_path='D:/360WiFi/FDDC_financial_data_20180613/'\n",
    "split_file_path='D:/360WiFi/files/financial_data_new/'\n",
    "input_path='D:/360WiFi/files/input/'\n",
    "mid_path='D:/360WiFi/files/mid_file/'\n",
    "other_path='D:/360WiFi/files/other_file/'\n",
    "\n",
    "'''\n",
    "\n",
    "preprocess(del_list):模块1最终执行函数，合并三大报表，生成 merge_file.csv\n",
    "\n",
    "generate_data_base():模块2最终执行函数将merge_file表进行添加“三级行业代码、指数代码和三级训练标准”，生成table_base.csv\n",
    "\n",
    "merge_3file():模块3执行函数，将table_base与Market_data,Company_operation,Macro data合并\n",
    "\n",
    "'''\n",
    "\n",
    "start_time='2009-01-01'\n",
    "\n",
    "\n",
    "\n",
    "#####-------------------------模块1：合三表-------------------------------------------        \n",
    "        \n",
    "###-------生成器读取文件-----------------\n",
    "# def loading_inputs():\n",
    "#     files= os.listdir(input_path+'/raw_data/')\n",
    "#     for file in files:\n",
    "#         print('reading file:%s'%file)\n",
    "#         feature = pd.read_csv(input_path+'/raw_data/'+file)\n",
    "#         yield feature\n",
    "# T=[]\n",
    "# for item in loading_inputs():\n",
    "#     T.append(item)\n",
    "\n",
    "\n",
    "###-------读取文件---------------------\n",
    "def loading_inputs():\n",
    "    files= os.listdir(input_path+'/raw_data/')\n",
    "    DF=[]\n",
    "    for file in files:\n",
    "        print('reading file:%s'%file)\n",
    "        feature = pd.read_csv(input_path+'/raw_data/'+file)\n",
    "        DF.append(feature)\n",
    "    \n",
    "    return DF\n",
    "\n",
    "\n",
    "def preprocess_common(del_list):\n",
    "    '''\n",
    "    输入是要删除的列，输出是删除指定列并去重的df\n",
    "    \n",
    "    '''\n",
    "    print('----------------------------------------------------')\n",
    "    print('Starting preprocessing....\\n')\n",
    "    print('=======================================================')\n",
    "    print('loading file...')\n",
    "    T=loading_inputs()\n",
    "\n",
    "    ##--------预处理:删列操作-------------------\n",
    "    print('=======================================================')\n",
    "    print('del some cols...')\n",
    "    for i in range(12):\n",
    "        print('del col of df')\n",
    "        try:\n",
    "            T[i]=T[i].drop(del_list[i][0].split(','),axis=1)\n",
    "        except Exception as err:\n",
    "            print(err)\n",
    "\n",
    "    ##-----------单表去重操作----------------------------\n",
    "\n",
    "    print('single sheet drop_duplicates...')\n",
    "\n",
    "    for i in range(12):\n",
    "        T[i]=utils.date_time_index(T[i]).drop_duplicates(subset=['TICKER_SYMBOL','END_DATE'], keep='last')\n",
    "\n",
    "    return T\n",
    "\n",
    "def preprocess_normal(T):\n",
    "    '''\n",
    "    处理正常的ticker,首先对三大表内部各自合四表，最后合三大表\n",
    "    \n",
    "    '''\n",
    "    print('=======================================================')\n",
    "    print('preprosessing the normal ticker...')\n",
    "    [balance_sheet_Bank_old,balance_sheet_GB_old,balance_sheet_Insurance_old,balance_sheet_Securities_old,\\\n",
    "    cashFlow_sheet_Bank_old,cashFlow_sheet_GB_old,cashFlow_sheet_Insurance_old,cashFlow_sheet_Securities_old,\\\n",
    "    income_sheet_Bank_old,income_sheet_GB_old,income_sheet_Insurance_old,income_sheet_Securities_old]=T\n",
    "    del T\n",
    "    gc.collect()\n",
    "\n",
    "    ###----------------预处理：添加类别列，找公共columns-----------------------\n",
    "    ##添加类别列\n",
    "\n",
    "    balance_sheet_Bank_old['category']='Bank'\n",
    "    cashFlow_sheet_Bank_old['category']='Bank'\n",
    "    income_sheet_Bank_old['category']='Bank'\n",
    "\n",
    "    balance_sheet_Securities_old['category']='Securities'\n",
    "    cashFlow_sheet_Securities_old['category']='Securities'\n",
    "    income_sheet_Securities_old['category']='Securities'\n",
    "\n",
    "    balance_sheet_Insurance_old['category']='Insurance'\n",
    "    cashFlow_sheet_Insurance_old['category']='Insurance'\n",
    "    income_sheet_Insurance_old['category']='Insurance'\n",
    "\n",
    "    balance_sheet_GB_old['category']='GB'\n",
    "    cashFlow_sheet_GB_old['category']='GB'\n",
    "    income_sheet_GB_old['category']='GB'\n",
    "\n",
    "\n",
    "    ##查找columns公共部分\n",
    "        #资产负债\n",
    "    A1=balance_sheet_Bank_old.columns\n",
    "    B1=balance_sheet_Securities_old.columns\n",
    "    C1=balance_sheet_Insurance_old.columns\n",
    "    D1=balance_sheet_GB_old.columns\n",
    "        #现金流量\n",
    "    A2=cashFlow_sheet_Bank_old.columns\n",
    "    B2=cashFlow_sheet_Securities_old.columns\n",
    "    C2=cashFlow_sheet_Insurance_old.columns\n",
    "    D2=cashFlow_sheet_GB_old.columns\n",
    "        #利润表\n",
    "    A3=income_sheet_Bank_old.columns\n",
    "    B3=income_sheet_Securities_old.columns\n",
    "    C3=income_sheet_Insurance_old.columns\n",
    "    D3=income_sheet_GB_old.columns\n",
    "\n",
    "\n",
    "    #确定最终放在表首的几列\n",
    "    common=['INDEX','END_DATE', 'END_DATE_REP','EXCHANGE_CD','FISCAL_PERIOD','MERGED_FLAG','PARTY_ID',\\\n",
    "     'PUBLISH_DATE','REPORT_TYPE','TICKER_SYMBOL','category']\n",
    "\n",
    "\n",
    "\n",
    "    ##分别对三表，找出每个表中四个类别公共部分\n",
    "\n",
    "    commom_balance=set(C1)&set(D1)&set(A1)&set(B1)#-set(common)\n",
    "    commom_cash=set(C2)&set(D2)&set(A2)&set(B2)#-set(common)\n",
    "    commom_income=set(C3)&set(D3)&set(A3)&set(B3)#-set(common)\n",
    "\n",
    "    commom_balance=list(commom_balance)\n",
    "    commom_cash=list(commom_cash)\n",
    "    commom_income=list(commom_income)\n",
    "\n",
    "    ##列出几个特殊ticker,暂时去掉\n",
    "\n",
    "    del_ticker=[563, 627, 712, 750, 776, 987, 2673, 600291, 600816]\n",
    "\n",
    "    T1=[balance_sheet_Bank_old,balance_sheet_GB_old,balance_sheet_Insurance_old,balance_sheet_Securities_old,\\\n",
    "     cashFlow_sheet_Bank_old,cashFlow_sheet_GB_old,cashFlow_sheet_Insurance_old,cashFlow_sheet_Securities_old,\\\n",
    "    income_sheet_Bank_old,income_sheet_GB_old,income_sheet_Insurance_old,income_sheet_Securities_old]\n",
    "\n",
    "    for i in range(12):\n",
    "        #选出不在列表中出现的ticker的数据\n",
    "        IN=np.logical_not(T1[i].TICKER_SYMBOL.isin(del_ticker))\n",
    "        T1[i]=T1[i][IN]\n",
    "\n",
    "    [balance_sheet_Bank_old,balance_sheet_GB_old,balance_sheet_Insurance_old,balance_sheet_Securities_old,\\\n",
    "     cashFlow_sheet_Bank_old,cashFlow_sheet_GB_old,cashFlow_sheet_Insurance_old,cashFlow_sheet_Securities_old,\\\n",
    "    income_sheet_Bank_old,income_sheet_GB_old,income_sheet_Insurance_old,income_sheet_Securities_old]=T1\n",
    "    del T1\n",
    "    gc.collect()    \n",
    "\n",
    "    ##----------读取对应表------------------------\n",
    "    \n",
    "    dic123=pd.read_csv(other_path+'C_N_dic.csv',encoding='gbk')\n",
    "    dic123=dict(zip(dic123.E_col,dic123.C_col))\n",
    "    \n",
    "    \n",
    "    ##--------------------资产负债表合表------------------------------\n",
    "\n",
    "    print('merging Balance sheet.....\\n')\n",
    "\n",
    "    #提取公共部分并concat\n",
    "\n",
    "    concat_list_balance=[balance_sheet_Bank_old[commom_balance],balance_sheet_Securities_old[commom_balance],\\\n",
    "            balance_sheet_Insurance_old[commom_balance],balance_sheet_GB_old[commom_balance]]\n",
    "\n",
    "    df_balance_common=pd.concat(concat_list_balance)\n",
    "\n",
    "    #将公共部分统一加后缀\n",
    "    old_common_column_balance=list(set(commom_balance)-set(common))\n",
    "    new_common_column_balance=[x+'_commom_balance' for x in old_common_column_balance]\n",
    "    \n",
    "    for item in old_common_column_balance:\n",
    "        dic123[item]=item+'_commom_balance'\n",
    "    \n",
    "    \n",
    "    df_balance_common.rename(columns=dict(zip(old_common_column_balance,new_common_column_balance)),inplace=True)\n",
    "    df_balance_common\n",
    "\n",
    "\n",
    "    ##依次提取并merge四表剩余部分\n",
    "\n",
    "    balance_df=[balance_sheet_Bank_old,balance_sheet_Securities_old,balance_sheet_Insurance_old,balance_sheet_GB_old]\n",
    "    name=['_balance_Bank','_balance_Securities','_balance_Insurance','_balance_GB']\n",
    "    balance_list=[]\n",
    "\n",
    "    for a,b in zip(balance_df,name):    \n",
    "        balance_bank_columns=list(set(a.columns)-set(commom_balance))\n",
    "        balance_bank_columns.extend(common)\n",
    "        t=set(balance_bank_columns)-set(common)\n",
    "        balance_bank_columns_new=[x+ b for x in t]\n",
    "        \n",
    "        for item in t:\n",
    "            dic123[item]=item+b\n",
    "        \n",
    "        \n",
    "        balance_sheet_Bank_old_rest=a[balance_bank_columns]\n",
    "        balance_sheet_Bank_old_rest.rename(columns=dict(zip(t,balance_bank_columns_new)),inplace=True)\n",
    "\n",
    "        balance_list.append(balance_sheet_Bank_old_rest)\n",
    "    '''\n",
    "     merge有一个大bug,左连接时，merge(a,b),如果a和b都有中有多行相同，如a中有 ee行 = ff行  ，而b中也有  cc行= dd 行  ，\n",
    "     且 ee=cc=ff=dd就会出现最终merge结果条数>a；或者说b中有两条与a中某条对应，亦是如此.\n",
    "  \n",
    "    '''   \n",
    "\n",
    "    merge1=pd.merge(df_balance_common,balance_list[0],how='left',on=common)\n",
    "    merge2=pd.merge(merge1,balance_list[1],how='left',on=common)\n",
    "    merge3=pd.merge(merge2,balance_list[2],how='left',on=common)\n",
    "    merge_balance=pd.merge(merge3,balance_list[3],how='left',on=common)\n",
    "\n",
    "    merge_balance\n",
    "\n",
    "    ##--------------------现金流量表合表------------------------------\n",
    "    print('merging CashFlow sheet.....\\n')\n",
    "\n",
    "    #提取公共部分并concat\n",
    "\n",
    "    concat_list_cashFlow=[cashFlow_sheet_Bank_old[commom_cash],cashFlow_sheet_Securities_old[commom_cash],\\\n",
    "    cashFlow_sheet_Insurance_old[commom_cash],cashFlow_sheet_GB_old[commom_cash]]\n",
    "    df_cashFlow_common=pd.concat(concat_list_cashFlow)\n",
    "\n",
    "    df_cashFlow_common\n",
    "\n",
    "    #将公共部分统一加后缀\n",
    "    old_common_column_cash=list(set(commom_cash)-set(common))\n",
    "    new_common_column_cash=[x+'_commom_cash' for x in old_common_column_cash]\n",
    "    \n",
    "    \n",
    "    for item in old_common_column_cash:\n",
    "        dic123[item]=item+'_commom_cash'\n",
    "        \n",
    "    df_cashFlow_common.rename(columns=dict(zip(old_common_column_cash,new_common_column_cash)),inplace=True)\n",
    "\n",
    "    df_cashFlow_common\n",
    "\n",
    "\n",
    "    ##依次提取并merge四表剩余部分\n",
    "\n",
    "    cash_df=[cashFlow_sheet_Bank_old,cashFlow_sheet_Securities_old,cashFlow_sheet_Insurance_old,cashFlow_sheet_GB_old]\n",
    "    name=['_cashFlow_Bank','_cashFlow_Securities','_cashFlow_Insurance','_cashFlow_GB']\n",
    "    cash_list=[]\n",
    "\n",
    "    for a,b in zip(cash_df,name):    \n",
    "        cash_bank_columns=list(set(a.columns)-set(commom_cash))\n",
    "        cash_bank_columns.extend(common)\n",
    "        t=set(cash_bank_columns)-set(common)\n",
    "        cash_bank_columns_new=[x+ b for x in t]\n",
    "        for item in t:\n",
    "            dic123[item]=item+b\n",
    "        \n",
    "        cash_sheet_Bank_old_rest=a[cash_bank_columns]\n",
    "        cash_sheet_Bank_old_rest.rename(columns=dict(zip(t,cash_bank_columns_new)),inplace=True)\n",
    "\n",
    "        cash_list.append(cash_sheet_Bank_old_rest)\n",
    "\n",
    "\n",
    "    merge1=pd.merge(df_cashFlow_common,cash_list[0],how='left',on=common)\n",
    "    merge2=pd.merge(merge1,cash_list[1],how='left',on=common)\n",
    "    merge3=pd.merge(merge2,cash_list[2],how='left',on=common)\n",
    "    merge_cash=pd.merge(merge3,cash_list[3],how='left',on=common)\n",
    "    merge_cash\n",
    "\n",
    "    ##--------------------利润表合表------------------------------\n",
    "    print('merging Income sheet.....\\n')\n",
    "\n",
    "    #提取公共部分并concat\n",
    "\n",
    "    concat_list_income=[income_sheet_Bank_old[commom_income],income_sheet_Securities_old[commom_income],\\\n",
    "    income_sheet_Insurance_old[commom_income],income_sheet_GB_old[commom_income]]\n",
    "    df_income_common=pd.concat(concat_list_income)\n",
    "\n",
    "    df_income_common\n",
    "\n",
    "    #将公共部分统一加后缀\n",
    "    old_common_column_income=list(set(commom_income)-set(common))\n",
    "    new_common_column_income=[x+'_commom_income' for x in old_common_column_income]\n",
    "    \n",
    "    for item in old_common_column_income:\n",
    "        dic123[item]=item+'_commom_income'\n",
    "        \n",
    "        \n",
    "    df_income_common.rename(columns=dict(zip(old_common_column_income,new_common_column_income)),inplace=True)\n",
    "\n",
    "    df_income_common\n",
    "\n",
    "\n",
    "    ##依次提取并merge四表剩余部分\n",
    "\n",
    "    income_df=[income_sheet_Bank_old,income_sheet_Securities_old,income_sheet_Insurance_old,income_sheet_GB_old]\n",
    "    name=['_income_Bank','_income_Securities','_income_Insurance','_income_GB']\n",
    "    income_list=[]\n",
    "\n",
    "    for a,b in zip(income_df,name):    \n",
    "        income_bank_columns=list(set(a.columns)-set(commom_income))\n",
    "        income_bank_columns.extend(common)\n",
    "        t=set(income_bank_columns)-set(common)\n",
    "        income_bank_columns_new=[x+ b for x in t]\n",
    "        \n",
    "        for item in t:\n",
    "            dic123[item]=item+b\n",
    "            \n",
    "            \n",
    "        income_sheet_Bank_old_rest=a[income_bank_columns]\n",
    "        income_sheet_Bank_old_rest.rename(columns=dict(zip(t,income_bank_columns_new)),inplace=True)\n",
    "\n",
    "        income_list.append(income_sheet_Bank_old_rest)\n",
    "\n",
    "    merge1=pd.merge(df_income_common,income_list[0],how='left',on=common)\n",
    "    merge2=pd.merge(merge1,income_list[1],how='left',on=common)\n",
    "    merge3=pd.merge(merge2,income_list[2],how='left',on=common)\n",
    "    merge_income=pd.merge(merge3,income_list[3],how='left',on=common)\n",
    "    merge_income\n",
    "\n",
    "\n",
    "    \n",
    "    A=[]\n",
    "    B=[]\n",
    "    for (a,b) in dic123.items():\n",
    "        A.append(a)\n",
    "        B.append(b)\n",
    "    DataFrame({'E_col':A,'trains_col':B}).to_csv(file_path+'C_N_Trans.csv',encoding='gbk',index=False)\n",
    "    \n",
    "    \n",
    "    ##----------------合三表-------------------------\n",
    "\n",
    "    print('merging final sheet.....\\n')\n",
    "    merge_balance=merge_balance.drop(['INDEX'],axis=1)\n",
    "    merge_cash=merge_cash.drop(['INDEX'],axis=1)\n",
    "    merge_income=merge_income.drop(['INDEX'],axis=1)\n",
    "    merge_income\n",
    "\n",
    "    M1=pd.merge(merge_balance,merge_cash,how='outer',on=['END_DATE','EXCHANGE_CD','FISCAL_PERIOD','MERGED_FLAG','PARTY_ID',\\\n",
    "                                         'REPORT_TYPE','TICKER_SYMBOL','category'])##index,publist_date h和 enddate_rep应该删掉！\n",
    "    M2=pd.merge(M1,merge_income,how='outer',on=['END_DATE','EXCHANGE_CD','FISCAL_PERIOD','MERGED_FLAG','PARTY_ID',\\\n",
    "                                         'REPORT_TYPE','TICKER_SYMBOL','category'])\n",
    "    \n",
    "    \n",
    "    print('preprosessing the normal ticker done!!!...\\n')\n",
    "    print('=======================================================')\n",
    "    return M2\n",
    "\n",
    "def preprocess_special(T):\n",
    "    \n",
    "    '''\n",
    "    处理有冲突的ticker,首先对三大表内部各自合四表，最后合三大表\n",
    "    \n",
    "    '''\n",
    "    print('preprosessing the special ticker...')\n",
    "    [balance_sheet_Bank_old,balance_sheet_GB_old,balance_sheet_Insurance_old,balance_sheet_Securities_old,\\\n",
    "     cashFlow_sheet_Bank_old,cashFlow_sheet_GB_old,cashFlow_sheet_Insurance_old,cashFlow_sheet_Securities_old,\\\n",
    "    income_sheet_Bank_old,income_sheet_GB_old,income_sheet_Insurance_old,income_sheet_Securities_old]=T\n",
    "    del T\n",
    "    gc.collect()\n",
    "\n",
    "    ###----------------预处理：添加类别列，找公共columns-----------------------\n",
    "    ##添加类别列\n",
    "\n",
    "    balance_sheet_Bank_old['category']='SSR'\n",
    "    cashFlow_sheet_Bank_old['category']='SSR'\n",
    "    income_sheet_Bank_old['category']='SSR'\n",
    "\n",
    "    balance_sheet_Securities_old['category']='SSR'\n",
    "    cashFlow_sheet_Securities_old['category']='SSR'\n",
    "    income_sheet_Securities_old['category']='SSR'\n",
    "\n",
    "    balance_sheet_Insurance_old['category']='SSR'\n",
    "    cashFlow_sheet_Insurance_old['category']='SSR'\n",
    "    income_sheet_Insurance_old['category']='SSR'\n",
    "\n",
    "    balance_sheet_GB_old['category']='SSR'\n",
    "    cashFlow_sheet_GB_old['category']='SSR'\n",
    "    income_sheet_GB_old['category']='SSR'\n",
    "\n",
    "\n",
    "\n",
    "    ##查找columns公共部分\n",
    "        #资产负债\n",
    "    A1=balance_sheet_Bank_old.columns\n",
    "    B1=balance_sheet_Securities_old.columns\n",
    "    C1=balance_sheet_Insurance_old.columns\n",
    "    D1=balance_sheet_GB_old.columns\n",
    "        #现金流量\n",
    "    A2=cashFlow_sheet_Bank_old.columns\n",
    "    B2=cashFlow_sheet_Securities_old.columns\n",
    "    C2=cashFlow_sheet_Insurance_old.columns\n",
    "    D2=cashFlow_sheet_GB_old.columns\n",
    "        #利润表\n",
    "    A3=income_sheet_Bank_old.columns\n",
    "    B3=income_sheet_Securities_old.columns\n",
    "    C3=income_sheet_Insurance_old.columns\n",
    "    D3=income_sheet_GB_old.columns\n",
    "\n",
    "\n",
    "    #确定最终放在表首的几列\n",
    "    common=['INDEX','END_DATE', 'END_DATE_REP','EXCHANGE_CD','FISCAL_PERIOD','MERGED_FLAG','PARTY_ID',\\\n",
    "     'PUBLISH_DATE','REPORT_TYPE','TICKER_SYMBOL','category']\n",
    "\n",
    "\n",
    "    ##分别对三表，找出每个表中四个类别公共部分\n",
    "\n",
    "    commom_balance=set(C1)&set(D1)&set(A1)&set(B1)#-set(common)\n",
    "    commom_cash=set(C2)&set(D2)&set(A2)&set(B2)#-set(common)\n",
    "    commom_income=set(C3)&set(D3)&set(A3)&set(B3)#-set(common)\n",
    "\n",
    "    commom_balance=list(commom_balance)\n",
    "    commom_cash=list(commom_cash)\n",
    "    commom_income=list(commom_income)\n",
    "\n",
    "\n",
    "\n",
    "    ##选出ticker条目\n",
    "    del_ticker=[563, 627, 712, 750, 776, 987, 2673, 600291, 600816]\n",
    "\n",
    "    T1=[balance_sheet_Bank_old,balance_sheet_GB_old,balance_sheet_Insurance_old,balance_sheet_Securities_old,\\\n",
    "     cashFlow_sheet_Bank_old,cashFlow_sheet_GB_old,cashFlow_sheet_Insurance_old,cashFlow_sheet_Securities_old,\\\n",
    "    income_sheet_Bank_old,income_sheet_GB_old,income_sheet_Insurance_old,income_sheet_Securities_old]\n",
    "\n",
    "    for i in range(12):\n",
    "        T1[i]=T1[i][T1[i].TICKER_SYMBOL.isin(del_ticker)]\n",
    "\n",
    "    [balance_sheet_Bank_old,balance_sheet_GB_old,balance_sheet_Insurance_old,balance_sheet_Securities_old,\\\n",
    "     cashFlow_sheet_Bank_old,cashFlow_sheet_GB_old,cashFlow_sheet_Insurance_old,cashFlow_sheet_Securities_old,\\\n",
    "    income_sheet_Bank_old,income_sheet_GB_old,income_sheet_Insurance_old,income_sheet_Securities_old]=T1\n",
    "\n",
    "    del T1\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    ##--------------------资产负债表合表------------------------------\n",
    "\n",
    "    print('merging Balance sheet.....\\n')\n",
    "\n",
    "\n",
    "    #提取公共部分并concat\n",
    "\n",
    "\n",
    "    concat_list_balance=[balance_sheet_Bank_old[commom_balance],balance_sheet_Securities_old[commom_balance],\\\n",
    "            balance_sheet_Insurance_old[commom_balance],balance_sheet_GB_old[commom_balance]]\n",
    "\n",
    "    df_balance_common=pd.concat(concat_list_balance)\n",
    "\n",
    "    #将公共部分统一加后缀\n",
    "    old_common_column_balance=list(set(commom_balance)-set(common))\n",
    "    new_common_column_balance=[x+'_commom_balance' for x in old_common_column_balance]\n",
    "\n",
    "    df_balance_common.rename(columns=dict(zip(old_common_column_balance,new_common_column_balance)),inplace=True)\n",
    "    df_balance_common\n",
    "\n",
    "\n",
    "\n",
    "    ##依次提取并merge四表剩余部分\n",
    "\n",
    "    balance_df=[balance_sheet_Bank_old,balance_sheet_Securities_old,balance_sheet_Insurance_old,balance_sheet_GB_old]\n",
    "    name=['_balance_Bank','_balance_Securities','_balance_Insurance','_balance_GB']\n",
    "    balance_list=[]\n",
    "\n",
    "    for a,b in zip(balance_df,name):    \n",
    "        balance_bank_columns=list(set(a.columns)-set(commom_balance))\n",
    "        balance_bank_columns.extend(common)\n",
    "        t=set(balance_bank_columns)-set(common)\n",
    "        balance_bank_columns_new=[x+ b for x in t]\n",
    "        balance_sheet_Bank_old_rest=a[balance_bank_columns]\n",
    "        balance_sheet_Bank_old_rest.rename(columns=dict(zip(t,balance_bank_columns_new)),inplace=True)\n",
    "\n",
    "        balance_list.append(balance_sheet_Bank_old_rest)\n",
    "\n",
    "    ##再次去重    \n",
    "    df_balance_common=df_balance_common.sort_values(by=['TICKER_SYMBOL','END_DATE','PUBLISH_DATE','END_DATE_REP'])\n",
    "    df_balance_common.drop_duplicates(subset=['TICKER_SYMBOL','END_DATE'],keep='last',inplace=True)                                                    \n",
    "\n",
    "\n",
    "    merge1=pd.merge(df_balance_common,balance_list[0],how='left',on=common)\n",
    "    merge2=pd.merge(merge1,balance_list[1],how='left',on=common)\n",
    "    merge3=pd.merge(merge2,balance_list[2],how='left',on=common)\n",
    "    merge_balance=pd.merge(merge3,balance_list[3],how='left',on=common)\n",
    "\n",
    "    merge_balance\n",
    "\n",
    "\n",
    "#     ##-------------检查concat后是否重复-------------------------\n",
    "\n",
    "#     t=df_balance_common.groupby(['TICKER_SYMBOL','END_DATE']).size()\n",
    "#     t=t[t!=1].reset_index()#.to_csv(file_path+'Balance_conflict.csv',index=False)\n",
    "\n",
    "\n",
    "#     def check_ssr(df):\n",
    "#     '''\n",
    "#     找出concat后，重复ticker-enddate，的条目数据，公共部分columns是否一致\n",
    "#     '''\n",
    "#         t=df.groupby(['TICKER_SYMBOL','END_DATE']).size()\n",
    "#         t=t[t!=1].reset_index()\n",
    "#         t.END_DATE=t.END_DATE.apply(lambda x:datetime.datetime.strftime(x,'%Y-%m-%d'))\n",
    "#         for m,n in zip(t.TICKER_SYMBOL,t.END_DATE):\n",
    "#             a=list(df.columns)\n",
    "#             a.remove('PUBLISH_DATE')\n",
    "#             a.remove('END_DATE_REP')\n",
    "#             a.remove('INDEX')\n",
    "#             if len(df[(df.TICKER_SYMBOL==m)&(df.END_DATE==n)].drop_duplicates(subset=a))==1:\n",
    "#                 print(m,n)#to_csv(file_path+'123.csv',index=False)#.drop_duplicates(subset=a)\n",
    "\n",
    "\n",
    "\n",
    "    ##--------------------现金流量表合表------------------------------\n",
    "    print('merging CashFlow sheet.....\\n')\n",
    "\n",
    "    #提取公共部分并concat\n",
    "\n",
    "    concat_list_cashFlow=[cashFlow_sheet_Bank_old[commom_cash],cashFlow_sheet_Securities_old[commom_cash],\\\n",
    "    cashFlow_sheet_Insurance_old[commom_cash],cashFlow_sheet_GB_old[commom_cash]]\n",
    "    df_cashFlow_common=pd.concat(concat_list_cashFlow)\n",
    "\n",
    "    df_cashFlow_common\n",
    "\n",
    "    #将公共部分统一加后缀\n",
    "    old_common_column_cash=list(set(commom_cash)-set(common))\n",
    "    new_common_column_cash=[x+'_commom_cash' for x in old_common_column_cash]\n",
    "    df_cashFlow_common.rename(columns=dict(zip(old_common_column_cash,new_common_column_cash)),inplace=True)\n",
    "\n",
    "    df_cashFlow_common\n",
    "\n",
    "\n",
    "\n",
    "    ##依次提取并merge四表剩余部分\n",
    "\n",
    "    cash_df=[cashFlow_sheet_Bank_old,cashFlow_sheet_Securities_old,cashFlow_sheet_Insurance_old,cashFlow_sheet_GB_old]\n",
    "    name=['_cashFlow_Bank','_cashFlow_Securities','_cashFlow_Insurance','_cashFlow_GB']\n",
    "    cash_list=[]\n",
    "\n",
    "    for a,b in zip(cash_df,name):    \n",
    "        cash_bank_columns=list(set(a.columns)-set(commom_cash))\n",
    "        cash_bank_columns.extend(common)\n",
    "        t=set(cash_bank_columns)-set(common)\n",
    "        cash_bank_columns_new=[x+ b for x in t]\n",
    "        cash_sheet_Bank_old_rest=a[cash_bank_columns]\n",
    "        cash_sheet_Bank_old_rest.rename(columns=dict(zip(t,cash_bank_columns_new)),inplace=True)\n",
    "\n",
    "        cash_list.append(cash_sheet_Bank_old_rest)\n",
    "\n",
    "    ##再次去重\n",
    "    df_cashFlow_common=df_cashFlow_common.sort_values(by=['TICKER_SYMBOL','END_DATE','PUBLISH_DATE','END_DATE_REP'])\n",
    "    df_cashFlow_common.drop_duplicates(subset=['TICKER_SYMBOL','END_DATE'],keep='last',inplace=True)       \n",
    "\n",
    "    merge1=pd.merge(df_cashFlow_common,cash_list[0],how='left',on=common)\n",
    "    merge2=pd.merge(merge1,cash_list[1],how='left',on=common)\n",
    "    merge3=pd.merge(merge2,cash_list[2],how='left',on=common)\n",
    "    merge_cash=pd.merge(merge3,cash_list[3],how='left',on=common)\n",
    "    merge_cash\n",
    "\n",
    "\n",
    "    ##--------------------利润表合表------------------------------\n",
    "    print('merging Income sheet.....\\n')\n",
    "\n",
    "    #提取公共部分并concat\n",
    "\n",
    "    concat_list_income=[income_sheet_Bank_old[commom_income],income_sheet_Securities_old[commom_income],\\\n",
    "    income_sheet_Insurance_old[commom_income],income_sheet_GB_old[commom_income]]\n",
    "    df_income_common=pd.concat(concat_list_income)\n",
    "\n",
    "    df_income_common\n",
    "\n",
    "    #将公共部分统一加后缀\n",
    "    old_common_column_income=list(set(commom_income)-set(common))\n",
    "    new_common_column_income=[x+'_commom_income' for x in old_common_column_income]\n",
    "    df_income_common.rename(columns=dict(zip(old_common_column_income,new_common_column_income)),inplace=True)\n",
    "\n",
    "    df_income_common\n",
    "\n",
    "\n",
    "    ##依次提取并merge四表剩余部分\n",
    "\n",
    "    income_df=[income_sheet_Bank_old,income_sheet_Securities_old,income_sheet_Insurance_old,income_sheet_GB_old]\n",
    "    name=['_income_Bank','_income_Securities','_income_Insurance','_income_GB']\n",
    "    income_list=[]\n",
    "\n",
    "    for a,b in zip(income_df,name):    \n",
    "        income_bank_columns=list(set(a.columns)-set(commom_income))\n",
    "        income_bank_columns.extend(common)\n",
    "        t=set(income_bank_columns)-set(common)\n",
    "        income_bank_columns_new=[x+ b for x in t]\n",
    "        income_sheet_Bank_old_rest=a[income_bank_columns]\n",
    "        income_sheet_Bank_old_rest.rename(columns=dict(zip(t,income_bank_columns_new)),inplace=True)\n",
    "\n",
    "        income_list.append(income_sheet_Bank_old_rest)\n",
    "\n",
    "\n",
    "    ##再次去重\n",
    "    df_income_common=df_income_common.sort_values(by=['TICKER_SYMBOL','END_DATE','PUBLISH_DATE','END_DATE_REP'])\n",
    "    df_income_common.drop_duplicates(subset=['TICKER_SYMBOL','END_DATE'],keep='last',inplace=True) \n",
    "\n",
    "\n",
    "    merge1=pd.merge(df_income_common,income_list[0],how='left',on=common)\n",
    "    merge2=pd.merge(merge1,income_list[1],how='left',on=common)\n",
    "    merge3=pd.merge(merge2,income_list[2],how='left',on=common)\n",
    "    merge_income=pd.merge(merge3,income_list[3],how='left',on=common)\n",
    "    merge_income\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ##----------------合三表-------------------------\n",
    "\n",
    "    print('merging final sheet.....\\n')\n",
    "    merge_balance=merge_balance.drop(['INDEX'],axis=1)\n",
    "    merge_cash=merge_cash.drop(['INDEX'],axis=1)\n",
    "    merge_income=merge_income.drop(['INDEX'],axis=1)\n",
    "    merge_income\n",
    "\n",
    "    M1=pd.merge(merge_balance,merge_cash,how='outer',on=['END_DATE','EXCHANGE_CD','FISCAL_PERIOD','MERGED_FLAG','PARTY_ID',\\\n",
    "                                         'REPORT_TYPE','TICKER_SYMBOL','category'])\n",
    "    M2=pd.merge(M1,merge_income,how='outer',on=['END_DATE','EXCHANGE_CD','FISCAL_PERIOD','MERGED_FLAG','PARTY_ID',\\\n",
    "                                         'REPORT_TYPE','TICKER_SYMBOL','category'])\n",
    "    M2\n",
    "    \n",
    "    print('preprosessing the special ticker done!!!...')\n",
    "    print('=======================================================')\n",
    "    \n",
    "    \n",
    "    return M2\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(del_list):\n",
    "    '''\n",
    "    生成财务合并表：merge_file.csv\n",
    "    '''\n",
    "    base_file=preprocess_common(del_list)\n",
    "    file_normal=preprocess_normal(base_file)\n",
    "    file_special=preprocess_special(base_file)\n",
    "    \n",
    "    merge_file=pd.concat([file_normal,file_special])\n",
    "    \n",
    "    common=['PARTY_ID','TICKER_SYMBOL', 'EXCHANGE_CD','PUBLISH_DATE','END_DATE_REP','END_DATE','FISCAL_PERIOD',\\\n",
    "        'REPORT_TYPE', 'MERGED_FLAG', 'category']\n",
    "    \n",
    "    merge_file=merge_file[common+list(set(merge_file.columns)-set(common))]\n",
    "    \n",
    "    \n",
    "    print('the shape of normal ticker file:',file_normal.shape)\n",
    "    print('the shape of special ticker file:',file_special.shape)\n",
    "    print('the shape of merge file:',merge_file.shape)\n",
    "    \n",
    "    merge_file.to_csv(mid_path+'merge_file.csv',index=False)\n",
    "    \n",
    "    \n",
    "    return merge_file\n",
    "    \n",
    "# ##再次check merge_balance,merge_cash，merge_income，M2\n",
    "# ###是否有'TICKER_SYMBOL'-'END_DATE'重复项\n",
    "# '''\n",
    "# 也就是比较drop前后长度是否相等，若相等，则正确\n",
    "# '''\n",
    "# def check_dup(DF):\n",
    "#     print(DF.shape)\n",
    "#     print(DF.drop_duplicates(subset=['TICKER_SYMBOL','END_DATE'], keep='last').shape)\n",
    "\n",
    "# check_dup(M2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###==========================================模块2：添加属性=================================================\n",
    "def tick_symbol_map():\n",
    "    '''\n",
    "    生成ticker_symbol_2006/2008文件\n",
    "    '''\n",
    "    \n",
    "    h=pd.read_csv(other_path+'level_1_3_industry_class_shenwan.csv',encoding='gbk')\n",
    "    h=h[['ticker','industryID','industrySymbol','intoDate','outDate','isNew']]\n",
    "    h['outDate']=h['outDate'].fillna('2018/3/30')\n",
    "\n",
    "    h['intoDate']=pd.to_datetime(h['intoDate'],format='%Y/%m/%d') \n",
    "    h['outDate']=pd.to_datetime(h['outDate'],format='%Y/%m/%d') \n",
    "    h=h[h['outDate']>='2006-01-01']\n",
    "    h=h.sort_values(by=['ticker','intoDate'])\n",
    "    h['intoDate']=h['intoDate'].apply(lambda x:datetime.datetime.strftime(x,format='%Y/%m/%d') )\n",
    "    h['outDate']=h['outDate'].apply(lambda x:datetime.datetime.strftime(x,format='%Y/%m/%d') )\n",
    "    h\n",
    "    month_list=[]\n",
    "    ticker_list=[]\n",
    "    symbol_list=[]\n",
    "    id_list=[]\n",
    "    for indexs in h.index:\n",
    "        month=getBetweenMonth(h.loc[indexs,'intoDate'],h.loc[indexs,'outDate'])\n",
    "        month_list.extend(month)\n",
    "        ticker=h.loc[indexs,'ticker']\n",
    "        ticker_list.extend([ticker for x in range(len(month))])\n",
    "        symbol=h.loc[indexs,'industrySymbol']\n",
    "        symbol_list.extend([symbol for x in range(len(month))])\n",
    "        id_=h.loc[indexs,'industryID']\n",
    "        id_list.extend([id_ for x in range(len(month))])\n",
    "\n",
    "    print(len(month_list))\n",
    "    print(len(ticker_list))\n",
    "    print(len(symbol_list))\n",
    "    print(len(id_list))\n",
    "    id_symbol=DataFrame({'TICKER_SYMBOL':ticker_list,'time':month_list,'industrySymbol_level_3':symbol_list,'industryID':id_list})\n",
    "    id_symbol.drop_duplicates(['TICKER_SYMBOL','time'], keep='last',inplace=True)\n",
    "    return id_symbol\n",
    "\n",
    "\n",
    "def merge_base():\n",
    "    '''\n",
    "    基于Market Data和ticker_symbol_2006，生成merge_base基准表。\n",
    "    \n",
    "    '''\n",
    "    #market_data=pd.read_csv(file_path+'/files/Market Data_20180529.csv',encoding='gbk')\n",
    "    market_data=pd.read_excel(other_path+'[New] Market Data_20180613.xlsx',encoding='gbk')\n",
    "    ticker_symbol=pd.read_csv(other_path6+'ticker_symbol_2006.csv',encoding='gbk')\n",
    "    market_data['END_DATE_']=pd.to_datetime(market_data['END_DATE_'],format='%Y/%m/%d') \n",
    "    market_data=market_data[['TICKER_SYMBOL','END_DATE_','TYPE_ID']]\n",
    "    market_data.rename(columns={'TYPE_ID':'industryID_level_3'},inplace=True)\n",
    "\n",
    "\n",
    "    market_data['year']=market_data['END_DATE_'].dt.year\n",
    "    market_data['month']=market_data['END_DATE_'].dt.month\n",
    "\n",
    "    market_data=market_data[market_data['END_DATE_']>='2008-01-01']\n",
    "    ticker_symbol['year']=ticker_symbol['time'].map(lambda x:'%s'%str(x)[0:4]).astype(int)\n",
    "    ticker_symbol['month']=ticker_symbol['time'].map(lambda x:'%s'%str(x)[4:]).astype(int)\n",
    "    ticker_symbol=ticker_symbol[ticker_symbol['TICKER_SYMBOL']!='DY600018']\n",
    "    ticker_symbol['TICKER_SYMBOL']=ticker_symbol['TICKER_SYMBOL'].astype(int)\n",
    "\n",
    "    ticker_symbol\n",
    "    test=pd.merge(ticker_symbol,market_data,how='left',on=['TICKER_SYMBOL','year','month'])\n",
    "    \n",
    "    ##以market data中symbol为准，如果有不同，则将industryID替换为market data中的id\n",
    "    index_=test[(test['industryID']!=test['industryID_level_3'])&(test['industryID_level_3'].isnull().values==False)].index#找出两列不相等值的地方\n",
    "    #test[(test['industryID_level_3'].isnull().values==False)]#选出某列不为空的记录\n",
    "    test.loc[index_,'industryID']=test.loc[index_,'industryID_level_3']\n",
    "\n",
    "    test.to_csv(file_path+'merge_base.csv',index=False)\n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "##生成 Bigtable表\n",
    "def gengerate_big_table():\n",
    "    '''\n",
    "    merge_base：包含公司从上市以来每个月对应的industry_ID,\n",
    "    ID_industry_index_Symbol:industry_ID对应的一二三级industry_symbol\n",
    "    \n",
    "    以上两者merge后，生成ticker_industry_record.csv,\n",
    "    \n",
    "    再和merge_file.csv（大表）merge生成Bigtable\n",
    "    '''\n",
    "\n",
    "    base=pd.read_csv(mid_path+'merge_base.csv',encoding='gbk')\n",
    "    base.drop(['industrySymbol_level_3','time','END_DATE_','industryID_level_3'],axis=1,inplace=True)\n",
    "    base.rename(columns={'industryID':'industryID_level_3'},inplace=True)\n",
    "\n",
    "\n",
    "    id_symbol=pd.read_csv(other_path+'ID_industry_index_Symbol.csv',encoding='gbk')\n",
    "    columns=['industrySymbol_level_1', 'industrySymbol_level_2', 'industrySymbol_level_3', 'indexSymbol_level_1',\\\n",
    "            'indexSymbol_level_2','indexSymbol_level_3']\n",
    "    for item in columns:\n",
    "        id_symbol[item]=id_symbol[item].fillna(0).astype(int)\n",
    "\n",
    "    tick_symbol_index=pd.merge(base,id_symbol,on=['industryID_level_3'])\n",
    "    \n",
    "    tick_symbol_index.to_csv(other_path+'ticker_industry_record.csv',index=False,encoding='gbk')\n",
    "    ###tick_symbol_index[tick_symbol_index['TICKER_SYMBOL']==600832].sort_values(by=['year','month'])\n",
    "\n",
    "    Table=pd.read_csv(mid_path+'merge_file.csv',encoding='gbk')\n",
    "\n",
    "    Table['END_DATE']=pd.to_datetime(Table['END_DATE'],format='%Y/%m/%d') \n",
    "\n",
    "    Table['year']=Table['END_DATE'].dt.year\n",
    "    Table['month']=Table['END_DATE'].dt.month\n",
    "\n",
    "\n",
    "    Bigtable=pd.merge(Table,tick_symbol_index,how='left',on=['TICKER_SYMBOL','year','month'])\n",
    "    Bigtable.drop(['year', 'month','level_1','level_2'],axis=1,inplace=True)#删除tick_symbol_index中一些无用特征\n",
    "\n",
    "\n",
    "    Bigtable[['industryID_level_3', 'industryID_level_1', 'industryName_level_1',\n",
    "           'industrySymbol_level_1', 'indexSymbol_level_1', 'industryID_level_2',\n",
    "           'industryName_level_2', 'industrySymbol_level_2', 'indexSymbol_level_2',\n",
    "           'industryName_level_3', 'industrySymbol_level_3', 'indexSymbol_level_3',\n",
    "           'isNew']]\n",
    "\n",
    "    columns=['industrySymbol_level_1','indexSymbol_level_1','industrySymbol_level_2',\\\n",
    "             'indexSymbol_level_2','industrySymbol_level_3', 'indexSymbol_level_3']\n",
    "    for item in columns:\n",
    "        Bigtable[item]=Bigtable[item].fillna(0).astype(int)\n",
    "    \n",
    "    return Bigtable\n",
    "\n",
    "\n",
    "class Table_base(object):\n",
    "    \n",
    "    '''\n",
    "    在Bigtable基础上，添加class_by_14L1,2,3三列\n",
    "    '''\n",
    "    def __init__(self,table):\n",
    "        self.Bigtable=table\n",
    "        pass\n",
    "        \n",
    "    '''\n",
    "    财报信息早于股市信息，所以有一些在财报发布年份，公司的具体行业代码尚不清楚。其中一部分可以根据level1_3完善(ticker_symbol)去解决，正在想办法。还有一部分暂时解决不了。\n",
    "    '''\n",
    "    #@staticmethod \n",
    "    def L1_classifi(self):\n",
    "        '''\n",
    "        一级分类操作\n",
    "        '''\n",
    "        print('adding 1st level classify columns ')\n",
    "        Bigtable=self.Bigtable\n",
    "        Bigtable['Mid_L2']=Bigtable['industrySymbol_level_2']\n",
    "        Bigtable['Mid_L3']=Bigtable['industrySymbol_level_3']\n",
    "        Bigtable['classifi_by_14L1']=Bigtable['industrySymbol_level_1']\n",
    "\n",
    "        index_1=Bigtable[(Bigtable['Mid_L2']==340200)|(Bigtable['Mid_L2']==340100)].index\n",
    "        index_1\n",
    "        Bigtable.loc[index_1,'Mid_L2']=Bigtable.loc[index_1,'Mid_L2'].map({340200:340400,340100:340400})\n",
    "        class_map_L1=pd.read_csv(file_path+'/files/value_map/classifi_by_14L1.csv')\n",
    "        class_map_L1\n",
    "        dic_14L1=dict(zip(class_map_L1.indusSymbol_L23_11,class_map_L1.indusSymbol_L1_14))\n",
    "        dic_14L1\n",
    "        for (a,b) in dic_14L1.items():\n",
    "            index_2=Bigtable[(Bigtable['Mid_L2']==a)|(Bigtable['Mid_L3']==a)].index\n",
    "            Bigtable.loc[index_2,'classifi_by_14L1']=b\n",
    "\n",
    "        Bigtable[(Bigtable['Mid_L2']==250100)|(Bigtable['Mid_L3']==310307)]\n",
    "        Bigtable.classifi_by_14L1.unique()\n",
    "        Bigtable[Bigtable['classifi_by_14L1']==0]\n",
    "        \n",
    "        return Bigtable\n",
    "\n",
    "    #@staticmethod \n",
    "    def map_08_11(self):\n",
    "        '''\n",
    "        实现08-11年的代码转换\n",
    "        '''\n",
    "        \n",
    "        Bigtable=self.Bigtable\n",
    "        #08-11一般替换\n",
    "        map_0811_basic=pd.read_csv(file_path+'/files/value_map/map_08_11_basic.csv',encoding='gbk')\n",
    "        map_0811_basic_dic=dict(zip(map_0811_basic['08_L3_symbol'],zip(map_0811_basic['11_L3_symbol'],map_0811_basic['11_L2_symbol'])))\n",
    "        map_0811_basic_dic.items()\n",
    "        for (a,(b,c)) in map_0811_basic_dic.items():\n",
    "            index_2=Bigtable[Bigtable['Mid_L3']==a].index\n",
    "            Bigtable.loc[index_2,'Mid_L3']=b\n",
    "            Bigtable.loc[index_2,'Mid_L2']=c\n",
    "\n",
    "        #08-11特殊替换\n",
    "        map_0811_specific=pd.read_csv(file_path+'/files/value_map/map_08_11_specific.csv',encoding='gbk')\n",
    "        map_0811_specific['ticker']=map_0811_specific['ticker'].fillna(0).astype(int)\n",
    "        map_0811_specific\n",
    "        map_0811_specific_dic=dict(zip(map_0811_specific['ticker'],zip(map_0811_specific['08_L3_symbol'],\\\n",
    "                                    zip(map_0811_specific['11_L3_symbol'],map_0811_specific['11_L2_symbol']))))\n",
    "        for (a,(b,(c,d))) in map_0811_specific_dic.items():\n",
    "            #sample:2263 220501 220501 220500\n",
    "            #print(a,b,c,d)\n",
    "            index_2=Bigtable[(Bigtable['END_DATE']<'2011-10-10')&(Bigtable['TICKER_SYMBOL']==a)&(Bigtable['Mid_L3']==b)].index\n",
    "            Bigtable.loc[index_2,'Mid_L3']=c\n",
    "            Bigtable.loc[index_2,'Mid_L2']=d\n",
    "\n",
    "        return Bigtable\n",
    "\n",
    "    #@staticmethod \n",
    "    def L2_classifi(self):\n",
    "        '''\n",
    "        二级分类操作\n",
    "        '''\n",
    "        print('adding 2nd level classify columns ')\n",
    "        Bigtable=self.L1_classifi()\n",
    "        Bigtable['Mid_L2']=Bigtable['industrySymbol_level_2']\n",
    "        Bigtable['Mid_L3']=Bigtable['industrySymbol_level_3']\n",
    "        Bigtable['classifi_by_14L2']=Bigtable['industrySymbol_level_2']\n",
    "\n",
    "        Bigtable=self.map_08_11()\n",
    "\n",
    "        #一般转换11-14\n",
    "        class_map_L2_basic=pd.read_csv(file_path+'/files/value_map/classifi_by_14L2_basic.csv',encoding='gbk')\n",
    "        class_map_L2_basic\n",
    "        dic_14L2_basic=dict(zip(class_map_L2_basic.indusSymbol_L3_11,class_map_L2_basic.indusSymbol_L2_14))\n",
    "        dic_14L2_basic.items()\n",
    "        for (a,b) in dic_14L2_basic.items():\n",
    "            index_2=Bigtable[(Bigtable['END_DATE']<='2014-01-01')&(Bigtable['Mid_L3']==a)].index\n",
    "            Bigtable.loc[index_2,'classifi_by_14L2']=b\n",
    "\n",
    "\n",
    "        #特殊转换11-14\n",
    "        class_map_L2_specific=pd.read_csv(file_path+'/files/value_map/classifi_by_14L2_specific.csv',encoding='gbk')\n",
    "        class_map_L2_specific\n",
    "        dic_14L2_specific=dict(zip(class_map_L2_specific.ticker,zip(class_map_L2_specific.indusSymbol_L3_11,class_map_L2_specific.indusSymbol_L2_14)))\n",
    "        dic_14L2_specific.items()\n",
    "        for (a,(b,c)) in dic_14L2_specific.items():\n",
    "            #sample:2081 250202 620200\n",
    "            #print(a,b,c)\n",
    "            index_2=Bigtable[(Bigtable['END_DATE']<='2014-01-01')&(Bigtable['TICKER_SYMBOL']==a)&(Bigtable['Mid_L3']==b)].index\n",
    "            Bigtable.loc[index_2,'classifi_by_14L2']=c\n",
    "        return Bigtable\n",
    "\n",
    "    #@staticmethod \n",
    "    def L3_classifi(self):\n",
    "\n",
    "        ##三级分类操作\n",
    "        print('adding 3rd level classify columns ')\n",
    "        Bigtable=self.L2_classifi()\n",
    "        Bigtable['Mid_L2']=Bigtable['industrySymbol_level_2']\n",
    "        Bigtable['Mid_L3']=Bigtable['industrySymbol_level_3']\n",
    "        Bigtable['classifi_by_14L3']=Bigtable['industrySymbol_level_3']\n",
    "\n",
    "        Bigtable=self.map_08_11()\n",
    "\n",
    "        #一般转换11-14\n",
    "        class_map_L3_basic=pd.read_csv(file_path+'/files/value_map/classifi_by_14L3_basic.csv',encoding='gbk')\n",
    "        class_map_L3_basic\n",
    "        dic_14L3_basic=dict(zip(class_map_L3_basic.indusSymbol_L3_11,class_map_L3_basic.indusSymbol_L3_14))\n",
    "        dic_14L3_basic.items()\n",
    "        for (a,b) in dic_14L3_basic.items():\n",
    "            #print(a,b)\n",
    "            index_2=Bigtable[(Bigtable['END_DATE']<='2014-01-01')&(Bigtable['Mid_L3']==a)].index\n",
    "            Bigtable.loc[index_2,'classifi_by_14L3']=b\n",
    "\n",
    "\n",
    "        #特殊转换11-14\n",
    "        class_map_L3_specific=pd.read_csv(file_path+'/files/value_map/classifi_by_14L3_specific.csv',encoding='gbk')\n",
    "        #先处理一下空格\n",
    "        class_map_L3_specific['ticker']=class_map_L3_specific['ticker'].apply(lambda x: 0 if x.isspace() else x)\n",
    "        class_map_L3_specific['ticker']=class_map_L3_specific['ticker'].astype(int)\n",
    "\n",
    "        dic_14L3_specific=zip(class_map_L3_specific.indusSymbol_L3_11,zip(class_map_L3_specific.ticker,class_map_L3_specific.indusSymbol_L3_14))\n",
    "        dic_14L3_specific\n",
    "\n",
    "        for (a,(b,c)) in dic_14L3_specific:\n",
    "            #sample:260404 2350 630402\n",
    "            #print(a,b,c)\n",
    "            index_2=Bigtable[(Bigtable['END_DATE']<='2014-01-01')&(Bigtable['TICKER_SYMBOL']==b)&(Bigtable['Mid_L3']==a)].index\n",
    "            Bigtable.loc[index_2,'classifi_by_14L3']=c\n",
    " \n",
    "\n",
    "        return  Bigtable\n",
    "\n",
    "\n",
    "\n",
    "# def drop_rows(T):\n",
    "#     '''\n",
    "#     去掉公司上市前信息\n",
    "    \n",
    "#     '''\n",
    "#     print('del some end_date before list_date...')\n",
    "#     early_date=pd.read_csv(other_path+'ticker_market.csv',encoding='gbk')\n",
    "#     early_date_dic=zip(early_date.TICKER_SYMBOL,early_date.listDate)\n",
    "\n",
    "#     T['END_DATE']=T['END_DATE'].apply(lambda x:datetime.datetime.strftime(x,format='%Y-%m-%d'))\n",
    "#     concat_list=[]\n",
    "#     for (a,b) in early_date_dic:\n",
    "#         concat_list.append(T[(T.TICKER_SYMBOL==a)&(T.END_DATE>=b)])\n",
    "\n",
    "#     T_new=pd.concat(concat_list)\n",
    "#     T_new['END_DATE']=pd.to_datetime(T_new['END_DATE'],format='%Y/%m/%d') \n",
    "#     T_new=T_new[T_new['END_DATE']>=start_time]\n",
    "    \n",
    "#     return T_new\n",
    "\n",
    "def generate_data_base():\n",
    "    '''\n",
    "    生成添加好三级行业对应，和三级分类标准的table_base.csv\n",
    "    '''\n",
    "    table=gengerate_big_table()\n",
    "    T=Table_base(table).L3_classifi()\n",
    "    #T=drop_rows(T)\n",
    "    T.to_csv(mid_path+'table_base.csv',index=False)\n",
    "    return T\n",
    "\n",
    "\n",
    "###======================================模块3：将赛放其余三表合并================================================\n",
    "\n",
    "def merge_3file():\n",
    "    '''\n",
    "    输入是合好的table_base.csv，输出是比赛给的六表合并,table_6_file.csv\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ###merge Market Data 数据\n",
    "\n",
    "    table_base=pd.read_csv(mid_path+'table_base.csv')\n",
    "    market_data=pd.read_excel(other_path+'[New] Market Data_20180613.xlsx')\n",
    "    market_data['year']=market_data['END_DATE_'].dt.year\n",
    "    market_data['month']=market_data['END_DATE_'].dt.month\n",
    "    market_data.columns\n",
    "\n",
    "    table_base['year']=table_base['END_DATE'].map(lambda x:'%s'%str(x)[0:4]).astype(int)\n",
    "    table_base['month']=table_base['END_DATE'].map(lambda x:'%s'%str(x)[5:7]).astype(int)\n",
    "    table_base\n",
    "\n",
    "    merge1=pd.merge(table_base,market_data[[ 'TICKER_SYMBOL', 'CLOSE_PRICE', 'TURNOVER_VOL', 'TURNOVER_VALUE', 'MARKET_VALUE',\\\n",
    "                                            'year', 'month']],how='left',on=['TICKER_SYMBOL','year','month'])\n",
    "    merge1\n",
    "\n",
    "\n",
    "    ###merge Macro Data 数据\n",
    "    macro_factor=pd.read_excel(other_path+'[New] Macro&Industry_20180613.xlsx',sheetname='指标信息',encoding='gbk')\n",
    "    macro_factor_list=macro_factor.INDIC_ID.astype(str).values\n",
    "\n",
    "    macro_data=pd.read_excel(other_path+'[New] Macro&Industry_20180613.xlsx',sheetname='数据信息',encoding='gbk')\n",
    "    macro_data=macro_data[macro_data.PERIOD_DATE>='2009-01-01']\n",
    "    macro_data.indic_id=macro_data.indic_id.astype(str)\n",
    "\n",
    "    df_list=[]\n",
    "\n",
    "    for factor in macro_factor_list:  \n",
    "        #print(factor)\n",
    "        t=macro_data[macro_data.indic_id==factor][['PERIOD_DATE','DATA_VALUE']]\n",
    "        t.rename(columns={'DATA_VALUE':factor},inplace=True)\n",
    "        df_list.append(t)\n",
    "    M=df_list[0]\n",
    "    for i in range(1,len(df_list)):\n",
    "        M=pd.merge(M,df_list[i],how='outer',on=['PERIOD_DATE'])\n",
    "    M.drop_duplicates(inplace=True)\n",
    "    M.to_csv(other_path+'macro_data.csv',index=False)\n",
    "\n",
    "\n",
    "    M['year']=M['PERIOD_DATE'].map(lambda x:'%s'%str(x)[0:4]).astype(int)\n",
    "    M['month']=M['PERIOD_DATE'].map(lambda x:'%s'%str(x)[5:7]).astype(int)\n",
    "    M\n",
    "    M['PERIOD_DATE']=pd.to_datetime(M['PERIOD_DATE'],format='%Y-%m-%d') \n",
    "    M.sort_values(by=['PERIOD_DATE'],inplace=True)\n",
    "    M.drop_duplicates(subset=['year','month'],inplace=True,keep='last')\n",
    "\n",
    "\n",
    "    merge2=pd.merge(merge1,M[M.columns[1:]],how='left',on=['year','month'])\n",
    "    merge2\n",
    "\n",
    "\n",
    "    ###merge Company operation Data 数据\n",
    "    company_operation=pd.read_excel(other_path+'[New] Company Operating_2018613.xlsx',sheetname='CN')\n",
    "    company_operation.rename(columns={'交易代码':'TICKER_SYMBOL','报告期':'END_DATE','值':'VALUE','指标':'INDIC_NAME_CN'},inplace=True)\n",
    "    company_operation.drop(company_operation.index[[0]],inplace=True)\n",
    "    company_operation.TICKER_SYMBOL=company_operation.TICKER_SYMBOL.astype(str)\n",
    "    company_operation['year']=company_operation['END_DATE'].dt.year.fillna(0).astype(int)\n",
    "    company_operation['month']=company_operation['END_DATE'].dt.month.fillna(0).astype(int)\n",
    "    company_operation=company_operation[company_operation.year>2008]\n",
    "    company_operation=company_operation[['TICKER_SYMBOL','VALUE','INDIC_NAME_CN','year','month']]\n",
    "\n",
    "    a=company_operation.groupby(['TICKER_SYMBOL','INDIC_NAME_CN']).size().reset_index()\n",
    "    a\n",
    "\n",
    "    ticker_INDIC=zip(a.TICKER_SYMBOL,a.INDIC_NAME_CN)\n",
    "    merge_list=[]\n",
    "\n",
    "    for (a,b) in ticker_INDIC:\n",
    "\n",
    "        t=company_operation[(company_operation['TICKER_SYMBOL']==a)&(company_operation['INDIC_NAME_CN']==b)]\n",
    "        t.rename(columns={'VALUE':a+'_'+b},inplace=True)\n",
    "        merge_list.append(t)\n",
    "\n",
    "    M=merge_list[0]\n",
    "    for i in range(1,len(merge_list)):\n",
    "        merge_list[i].drop(['INDIC_NAME_CN'],axis=1,inplace=True)\n",
    "        M=pd.merge(M,merge_list[i],how='outer',on=['TICKER_SYMBOL','year','month'])\n",
    "\n",
    "    M.drop(['INDIC_NAME_CN'],axis=1,inplace=True)\n",
    "    M\n",
    "    M.TICKER_SYMBOL=M.TICKER_SYMBOL.astype(int)\n",
    "\n",
    "    merge3=pd.merge(merge2,M,how='left',on=['TICKER_SYMBOL','year','month'])\n",
    "    merge3\n",
    "\n",
    "\n",
    "    merge3.to_csv(mid_path+'table_6_file.csv',index=False)\n",
    "    \n",
    "    print('the shape of 6_sheet merge_file:',merge3.shape)\n",
    "    \n",
    "    return \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
