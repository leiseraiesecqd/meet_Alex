{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Ipynb_importer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import utils,preprocess,merge\n",
    "import time\n",
    "import datetime\n",
    "import os,gc\n",
    "import math\n",
    "file_path='D:/360WiFi/'\n",
    "raw_data_path='D:/360WiFi/FDDC_financial_data_20180613/'\n",
    "split_file_path='D:/360WiFi/files/financial_data_new/'\n",
    "input_path='D:/360WiFi/files/input/'\n",
    "mid_path='D:/360WiFi/files/mid_file/'\n",
    "other_path='D:/360WiFi/files/other_file/'\n",
    "\n",
    "\n",
    "##------------------合表前，初筛列开关--------------------------------------------------------------\n",
    "#资产负债表\n",
    "del_col_Balance_Bank=['AE,AA,LE,LA,SEE,SEA,OTH_EFFECT_SE,OTH_EFFECT_SA,LEE,LEA']\n",
    "del_col_Balance_GB=['CAE,CAA,NCAE,NCAA,AE,AA,CLE,CLA,NCLE,NCLA,LE,LA,SEE,SEA,OTH_EFFECT_SE,OTH_EFFECT_SA,LEE,LEA']\n",
    "del_col_Balance_Insurance=['AE,AA,LE,LA,SEE,SEA,OTH_EFFECT_SE,OTH_EFFECT_SA,LEE,LEA']\n",
    "del_col_Balance_Securities=['AE,AA,LE,LA,SEE,SEA,OTH_EFFECT_SE,OTH_EFFECT_SA,LEE,LEA']\n",
    "\n",
    "#现金流量表\n",
    "del_col_cashFlow_Bank=['SPEC_OCIF,AOCIF,SPEC_OCOF,AOCOF,ANOCF,SPEC_ICIF,AICIF,SPEC_ICOF,AICOF,ANICF,SPEC_FCIF,AFCIF,SPEC_FCOF,AFCOF,ANFCF,OTH_EFFECT_CE,ACE,OTH_EFFECT_CEI,ACEI']\n",
    "del_col_cashFlow_GB=['SPEC_OCIF,AOCIF,SPEC_OCOF,AOCOF,ANOCF,SPEC_ICIF,AICIF,SPEC_ICOF,AICOF,ANICF,SPEC_FCIF,AFCIF,SPEC_FCOF,AFCOF,ANFCF,OTH_EFFECT_CE,ACE,OTH_EFFECT_CEI,ACEI']\n",
    "del_col_cashFlow_Insurance=['SPEC_OCIF,AOCIF,SPEC_OCOF,AOCOF,ANOCF,SPEC_ICIF,AICIF,SPEC_ICOF,AICOF,ANICF,SPEC_FCIF,AFCIF,SPEC_FCOF,AFCOF,ANFCF,OTH_EFFECT_CE,ACE,OTH_EFFECT_CEI,ACEI']\n",
    "del_col_cashFlow_Securities=['SPEC_OCIF,AOCIF,SPEC_OCOF,AOCOF,ANOCF,SPEC_ICIF,AICIF,SPEC_ICOF,AICOF,ANICF,SPEC_FCIF,AFCIF,SPEC_FCOF,AFCOF,ANFCF,OTH_EFFECT_CE,ACE,OTH_EFFECT_CEI,ACEI']\n",
    "\n",
    "\n",
    "#利润表\n",
    "del_col_Income_Bank=['SPEC_OR,AOR,SPEC_OC,AOC,OTH_EFFECT_OP,AE_EFFECT_OP,OTH_EFFECT_TP,AE_EFFECT_TP,OTH_EFFECT_NP,AE_EFFECT_NP,OTH_EFFECT_NPP,AE_EFFECT_NPP,OTH_EFFECT_CI,AE_EFFECT_CI,OTH_EFFECT_PCI,AE_EFFECT_PCI']\n",
    "del_col_Income_GB=['SPEC_TOR,ATOR,SPEC_TOC,ATOC,OTH_EFFECT_OP,AE_EFFECT_OP,OTH_EFFECT_TP,AE_EFFECT_TP,OTH_EFFECT_NP,AE_EFFECT_NP,OTH_EFFECT_NPP,AE_EFFECT_NPP,OTH_EFFECT_CI,AE_EFFECT_CI,OTH_EFFECT_PCI,AE_EFFECT_PCI']\n",
    "del_col_Income_Insurance=['SPEC_OR,AOR,SPEC_OC,AOC,OTH_EFFECT_OP,AE_EFFECT_OP,OTH_EFFECT_TP,AE_EFFECT_TP,OTH_EFFECT_NP,AE_EFFECT_NP,OTH_EFFECT_NPP,AE_EFFECT_NPP,OTH_EFFECT_CI,AE_EFFECT_CI,OTH_EFFECT_PCI,AE_EFFECT_PCI']\n",
    "del_col_Income_Securities=['SPEC_OR,AOR,SPEC_OC,AOC,OTH_EFFECT_OP,AE_EFFECT_OP,OTH_EFFECT_TP,AE_EFFECT_TP,OTH_EFFECT_NP,AE_EFFECT_NP,OTH_EFFECT_NPP,AE_EFFECT_NPP,OTH_EFFECT_CI,AE_EFFECT_CI,OTH_EFFECT_PCI,AE_EFFECT_PCI']\n",
    "\n",
    "\n",
    "\n",
    "del_list=[del_col_Balance_Bank,del_col_Balance_GB,del_col_Balance_Insurance,del_col_Balance_Securities,\\\n",
    "         del_col_cashFlow_Bank,del_col_cashFlow_GB,del_col_cashFlow_Insurance,del_col_cashFlow_Securities,\\\n",
    "        del_col_Income_Bank,del_col_Income_GB,del_col_Income_Insurance,del_col_Income_Securities]\n",
    "\n",
    "\n",
    "##------------------合表后，删列/删行开关-------------------------------------\n",
    "\n",
    "class_option='classifi_by_14L1'\n",
    "\n",
    "\n",
    "##'END_DATE'会在函数中删除，'REVENUE_commom_income'作为特征,暂不删除\n",
    "\n",
    "del_col=['PARTY_ID','EXCHANGE_CD', 'PUBLISH_DATE','END_DATE_REP',\\\n",
    "          'REPORT_TYPE','MERGED_FLAG', 'category',\\\n",
    "        'industryName_level_1','industryName_level_2','industryName_level_3','isNew','Mid_L2','Mid_L3',\\\n",
    "         'PUBLISH_DATE_y','PUBLISH_DATE_x','END_DATE_REP_x','END_DATE_REP_y','T_REVENUE_income_GB'\\\n",
    "        ]\n",
    "\n",
    "\n",
    "'''\n",
    "ticker_option的value中：第一项是起始时间，第二项是结束时间，第三项是要删的表，有四种选项['income','balance','cash','all']\n",
    "\n",
    "'''\n",
    "ticker_option={'601360':['2009-03-31','2018-03-31','all'],'000526':['2009-03-31','2018-03-31','all'],\n",
    "               '000616':['2009-03-31','2018-03-31','all'],'002113':['2009-03-31','2018-03-31','all'],\n",
    "               '000691':['2009-03-31','2018-03-31','all'],'300268':['2009-03-31','2018-03-31','all'],\n",
    "               '000693':['2009-03-31','2018-03-31','all'],'600149':['2009-03-31','2018-03-31','all'],\n",
    "               '000697':['2009-03-31','2018-03-31','all'],'600234':['2009-03-31','2018-03-31','all'],\n",
    "               '000892':['2009-03-31','2018-03-31','all'],'600766':['2009-03-31','2018-03-31','all'],\n",
    "               '000979':['2009-03-31','2018-03-31','all'],'600892':['2009-03-31','2018-03-31','all'],\n",
    "               '600149':['2009-03-31','2018-03-31','all'],'600193':['2009-03-31','2018-03-31','all'],\n",
    "               '600555':['2009-03-31','2018-03-31','all'],'600696':['2009-03-31','2018-03-31','all'],\n",
    "               '600732':['2009-03-31','2018-03-31','all'],'600747':['2009-03-31','2018-03-31','all'],\n",
    "               '600753':['2009-03-31','2018-03-31','all'],'600870':['2009-03-31','2018-03-31','all'],\\\n",
    "                '000030':['2009-03-31','2011-12-31','all'],\n",
    "               '000038':['2009-03-31','2009-09-30','all'],\n",
    "               '000048':['2017-12-31','2018-03-31','all'],\n",
    "               '000409':['2009-03-31','2011-03-31','all'],\n",
    "               '000498':['2009-03-31','2011-09-30','all'],\n",
    "               '000555':['2009-03-31','2009-12-31','all'],\n",
    "               '000557':['2009-03-31','2012-09-30','all'],  \n",
    "                '000563':['2009-03-31','2009-12-31','all'],\n",
    "               '000567':['2009-03-31','2016-03-31','all'],\n",
    "               '000587':['2009-03-31','2011-06-30','all'],\n",
    "               '000620':['2009-03-31','2010-03-31','all'],\n",
    "               '000638':['2009-03-31','2012-12-31','all'],\n",
    "               '000665':['2009-03-31','2009-09-30','all'],\n",
    "               '000673':['2009-03-31','2009-09-30','all'],\n",
    "                '000681':['2009-03-31','2013-03-31','all'],\n",
    "               '000688':['2009-03-31','2011-09-30','all'],\n",
    "               '000719':['2009-03-31','2010-03-31','all'],\n",
    "               '000796':['2009-03-31','2009-12-31','all'],\n",
    "               '000838':['2009-03-31','2013-09-30','all'],\n",
    "               '000856':['2009-03-31','2010-12-31','all'],\n",
    "               '000939':['2017-12-31','2018-03-31','all'],          \n",
    "               '000975':['2009-03-31','2013-06-30','all'],\n",
    "               '002061':['2016-12-31','2018-03-31','all'],\n",
    "               '002072':['2015-03-31','2018-03-31','all'],\n",
    "               '002175':['2017-12-31','2018-03-31','all'],\n",
    "               '002496':['2016-12-31','2018-03-31','all'],\n",
    "               '002499':['2017-06-30','2018-03-31','all'],\n",
    "               '002506':['2010-12-31','2014-09-30','all'],  \n",
    "                '002575':['2016-12-31','2018-03-31','all'],\n",
    "               '300028':['2009-03-31','2016-09-30','all'],\n",
    "               '300189':['2009-03-31','2017-09-30','all'],\n",
    "               '600074':['2009-03-31','2017-06-30','all'],\n",
    "               '600083':['2009-03-31','2012-03-31','all'],\n",
    "               '600094':['2009-03-31','2009-09-30','all'],\n",
    "               '600145':['2012-06-30','2018-03-31','all'],\n",
    "                '600180':['2009-03-31','2011-06-30','all'],\n",
    "               '600399':['2017-12-31','2018-03-31','all'],\n",
    "               '600485':['2013-09-30','2018-03-31','all'],\n",
    "               '600540':['2009-03-31','2017-09-30','all'],\n",
    "               '600556':['2009-03-31','2012-09-30','all'],\n",
    "               '600576':['2009-03-31','2015-06-30','all'],\n",
    "               '600603':['2009-03-31','2012-09-30','all'],\n",
    "                '600610':['2014-09-30','2018-03-31','all'],\n",
    "               '600617':['2009-03-31','2012-09-30','all'],\n",
    "               '600699':['2009-03-31','2010-09-30','all'],\n",
    "               '600705':['2009-03-31','2011-03-31','all'],\n",
    "               '600715':['2009-03-31','2015-06-30','all'],\n",
    "               '600769':['2009-03-31','2013-06-30','all'],\n",
    "               '600791':['2009-03-31','2010-12-31','all'],\n",
    "               '600817':['2015-12-31','2018-03-31','all'],\n",
    "               '600988':['2009-03-31','2009-03-31','all'],         \n",
    "              }\n",
    "\n",
    "##--------------one_hot开关------------------------------------\n",
    "\n",
    "oht_col=['TICKER_SYMBOL','FISCAL_PERIOD','industrySymbol_level_1','industrySymbol_level_2','industrySymbol_level_3',\\\n",
    "        'indexSymbol_level_1','indexSymbol_level_2','indexSymbol_level_3',\\\n",
    "         'industryID_level_1','industryID_level_2','industryID_level_3',\n",
    "        ]\n",
    "#merge.preprocess(del_list)\n",
    "# merge.generate_data_base()\n",
    "# merge.merge_3file()\n",
    "# preprocess.select_table(tabel,class_option,del_col,ticker_option)\n",
    "# preprocess.make_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "T=pd.read_csv(mid_path+'table_6_file.csv')\n",
    "T[T.TICKER_SYMBOL==601878]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macro_data=pd.read_excel(other_path+'[New] Macro&Industry_20180613.xlsx',sheetname='数据信息')\n",
    "# macro_data.indic_id.unique()\n",
    "\n",
    "# macro_data['year']=macro_data['PERIOD_DATE'].dt.year\n",
    "# macro_data['month']=macro_data['PERIOD_DATE'].dt.month\n",
    "\n",
    "# season_m=DataFrame({'month':[1,2,3,4,5,6,7,8,9,10,11,12],'fiscal_porid':['Q1','Q1','Q1','S','S','S','Q3','Q3','Q3','A','A','A']})\n",
    "# season_m\n",
    "\n",
    "\n",
    "# macro_data=pd.merge(macro_data,season_m,how='left',on=['month'])\n",
    "\n",
    "\n",
    "# marco_id_list=[1020000004, 1020000008, 1020001544, 1030000014, 1030000016,\n",
    "#        1030000018, 1030000020, 1040000046, 1040000050, 1040001767,\n",
    "#        1040002190, 1050000026, 1050000027, 1070000033, 1070000035,\n",
    "#        1070000039, 1080000235, 1090000363, 1090000365, 1100000874,\n",
    "#        1100002293, 1100005542, 1100006640, 1170000018, 1170000598,\n",
    "#        1170000618, 1170000641,1170005933, 1170007422,]\n",
    "\n",
    "# macro_data=macro_data[macro_data.indic_id.isin(marco_id_list)]\n",
    "# macro_data_M=macro_data[macro_data.FREQUENCY_CD=='M']\n",
    "# macro_data_Y=macro_data[macro_data.FREQUENCY_CD=='A']\n",
    "# macro_data_D=macro_data[macro_data.FREQUENCY_CD=='D']\n",
    "# macro_data_W=macro_data[macro_data.FREQUENCY_CD=='W']\n",
    "\n",
    "# macro_data_Y\n",
    "\n",
    "\n",
    "\n",
    "# merge_base=pd.read_csv(other_path+'macro_timeline.csv')\n",
    "# merge_base\n",
    "\n",
    "# ##添加Macro中周期为年的指标\n",
    "\n",
    "# for item in macro_data_Y.indic_id.unique():\n",
    "#     merge_base=pd.merge(merge_base,macro_data_Y[macro_data_Y.indic_id==item][['year','DATA_VALUE']],how='left',on=['year'])\n",
    "#     merge_base.rename(columns={'DATA_VALUE':'%d'%item},inplace=True)\n",
    "# merge_base\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##添加Macro中周期为月，日，星期的指标的指标\n",
    "\n",
    "# def calculate(df,merge_df,choose):\n",
    "    \n",
    "#     if choose=='add':\n",
    "#         T=df.groupby(['indic_id','year','fiscal_porid']).sum().reset_index()\n",
    "#     if choose=='mean':\n",
    "#         T=df.groupby(['indic_id','year','fiscal_porid']).mean().reset_index()\n",
    "#     for item in T.indic_id.unique():\n",
    "#         merge_df=pd.merge(merge_df,T[T.indic_id==item][['year','DATA_VALUE','fiscal_porid']],how='left',on=['year','fiscal_porid'])\n",
    "#         merge_df.rename(columns={'DATA_VALUE':'%d'%item},inplace=True)\n",
    "        \n",
    "#     return merge_df\n",
    "    \n",
    "\n",
    "# merge_base=calculate(macro_data_M,merge_base,'mean')\n",
    "# merge_base=calculate(macro_data_D,merge_base,'mean')\n",
    "# merge_base=calculate(macro_data_W,merge_base,'mean')\n",
    "\n",
    "merge_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##---------------生成宏观数据特征字典---------------------------------\n",
    "\n",
    "# macro_data=pd.read_excel(mid_path+'Macro_wind.xlsx',sheet_name='季度',header=None)\n",
    "# macro_data.drop(0,axis=1,inplace=True)\n",
    "# dic1=DataFrame({'table':macro_data.loc[0,:],'C_col':macro_data.loc[1,:],'E_col':['macro_%d'%x for x in range(len(macro_data.loc[0,:]))]})\n",
    "# dic1\n",
    "\n",
    "\n",
    "# macro_data=pd.read_excel(mid_path+'Macro_wind.xlsx',sheet_name='月',header=None)\n",
    "# macro_data.drop(0,axis=1,inplace=True)\n",
    "\n",
    "# dic2=DataFrame({'table':macro_data.loc[0,:],'C_col':macro_data.loc[1,:],'E_col':['macro_%d'%(x+219) for x in range(len(macro_data.loc[0,:]))]})\n",
    "# dic2\n",
    "\n",
    "\n",
    "# macro_data=pd.read_excel(mid_path+'Macro_wind.xlsx',sheet_name='周',header=None)\n",
    "# macro_data.drop(0,axis=1,inplace=True)\n",
    "# dic3=DataFrame({'table':macro_data.loc[0,:],'C_col':macro_data.loc[1,:],'E_col':['macro_%d'%(x+373) for x in range(len(macro_data.loc[0,:]))]})\n",
    "# dic3\n",
    "\n",
    "\n",
    "# macro_data=pd.read_excel(mid_path+'Macro_wind.xlsx',sheet_name='日',header=None)\n",
    "# macro_data.drop(0,axis=1,inplace=True)\n",
    "\n",
    "# dic4=DataFrame({'table':macro_data.loc[0,:],'C_col':macro_data.loc[1,:],'E_col':['macro_%d'%(x+376) for x in range(len(macro_data.loc[0,:]))]})\n",
    "# dic4\n",
    "\n",
    "\n",
    "# macro_data=pd.read_excel(other_path+'[New] Macro&Industry_20180613.xlsx')\n",
    "# macro_data   \n",
    "\n",
    "# dic5=DataFrame({'table':['赛方宏观' for i in range(len(macro_data.NAME_CN))],'C_col':macro_data.NAME_CN,'E_col':macro_data.INDIC_ID})\n",
    "# dic5\n",
    "\n",
    "\n",
    "# pd.concat([dic1,dic2,dic3,dic4,dic5]).to_csv(other_path+'feature_dic_macro.csv',index=False,encoding='gbk')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# table=pd.read_csv(mid_path+'table_6_file_label_new.csv')\n",
    "\n",
    "# Index=table[((table['TICKER_SYMBOL']>=200000)&(table['TICKER_SYMBOL']<300000))|((table['TICKER_SYMBOL']>=900000)&(table['TICKER_SYMBOL']<901000))].index\n",
    "# table.drop(Index,inplace=True)\n",
    "\n",
    "# #***********************  以2_开头和以900_开头的ticke共计 18个 **********\n",
    "# [200053, 200054, 200152, 200160, 200168, 200468, 200512, 200706,\n",
    "#        200771, 200986, 200992, 900929, 900939, 900948, 900951, 900953,\n",
    "#        900956, 900957]\n",
    "\n",
    "# #************************************************************************\n",
    "#table=pd.read_csv(file_path+'index_info_all.csv',encoding='gbk')\n",
    "#table[table['ticker']==801890]\n",
    "\n",
    "\n",
    "#A=pd.read_csv(mid_path+'table_6_file_label_new.csv')\n",
    "#A[np.isnan(A['indexSymbol_level_3'])]\n",
    "#sorted(A['industryID_level_1'].unique())\n",
    "\n",
    "#A[A['indexSymbol_level_3']==851911]\n",
    "\n",
    "#A[A['industrySymbol_level_3']==630302]\n",
    "\n",
    "\n",
    "w=A[['TICKER_SYMBOL','END_DATE','indexSymbol_level_1','indexSymbol_level_2','indexSymbol_level_3','industryID_level_1','industryID_level_2','industryID_level_3']]\n",
    "ww=w[(w['indexSymbol_level_3']==0)|(w['indexSymbol_level_2']==0)]\n",
    "#w[w.TICKER_SYMBOL==563]\n",
    "#index=pd.read_csv(file_path+'indusrty_envalue_all.csv',encoding='gbk')\n",
    "ww\n",
    "#table[table['ticker']==801190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=pd.read_csv(mid_path+'table_6_file_label_new.csv')\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T.groupby(['TICKER_SYMBOL','classifi_by_14L1']).size().reset_index().groupby(['classifi_by_14L1']).sum().reset_index().to_csv(file_path+'check_1.csv',index=False)\n",
    "\n",
    "#.to_csv(file_path+'check.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "生成初步跑的大表（官方数据）\n",
    "\n",
    "'''\n",
    "\n",
    "table=pd.read_csv(mid_path+'feature_importance_2.csv')\n",
    "table.get_dtype_counts()\n",
    "t=table.infer_objects().dtypes\n",
    "t[t=='int64']\n",
    "\n",
    "table['era']=table['year'].astype(str)+table['month'].apply(lambda x:\"%02d\"%x)\n",
    "table[['era','year','month']]\n",
    "\n",
    "T=pd.get_dummies(table,columns=['FISCAL_PERIOD'])\n",
    "T.get_dtype_counts()\n",
    "header=['era','FISCAL_PERIOD_3', 'FISCAL_PERIOD_6', 'FISCAL_PERIOD_9', 'FISCAL_PERIOD_12']\n",
    "T=T[header+list((set(T.columns)-set(header)))]\n",
    "T\n",
    "T.get_dtype_counts()\n",
    "\n",
    "dangji=pd.read_csv(mid_path+'table_6_file_label.csv')\n",
    "dangji.rename(columns={'label':'Revenue_now'},inplace=True)\n",
    "\n",
    "merge=pd.merge(T,dangji[['TICKER_SYMBOL','year','month','Revenue_now']],how='left',on=['TICKER_SYMBOL','year','month'])\n",
    "rr=merge[['TICKER_SYMBOL','year','month','Revenue_now','label']]\n",
    "rr[rr.year!=2018].to_csv(other_path+'check_revenue_prerevenue.csv',index=False)\n",
    "\n",
    "merge.drop(['year','month','TICKER_SYMBOL','REVENUE_commom_income'],axis=1,inplace=True)\n",
    "table\n",
    "merge.get_dtype_counts()\n",
    "merge[merge.date!='201803'].to_csv(other_path+'feature_importance_2_update.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table=pd.read_csv(mid_path+'table_6_file_label_new.csv')\n",
    "columns=table.columns\n",
    "columns[columns.str.contains('PARTY_ID')][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C_N_dic():\n",
    "\n",
    "\n",
    "    sheetname=['bs_indu','bs_bank','bs_secu','bs_insu','is_indu','is_bank','is_secu','is_insu','cf_indu','cf_bank','cf_secu','cf_insu']\n",
    "\n",
    "    E_col=[]\n",
    "    C_col=[]\n",
    "    source=[]\n",
    "    for name in sheetname:\n",
    "        C_N=pd.read_excel(other_path+'Data Dictionary_5.29.xlsx',sheetname=name)\n",
    "        t=C_N.drop([0,1,2,3])\n",
    "        E_col.extend(t.iloc[:,0].values)\n",
    "        C_col.extend(t.iloc[:,2].values)\n",
    "        source.extend([name for i in range(len(t.iloc[:,2].values))])\n",
    "\n",
    "    C_N=DataFrame({'E_col':E_col,'C_col':C_col,'source':source})\n",
    "    C_N.drop_duplicates(subset=['E_col'],inplace=True)\n",
    "\n",
    "    ##注意一下COGS\n",
    "\n",
    "    #选取第几列\n",
    "    #t.iloc[:,[0,2]]\n",
    "\n",
    "    C_N.to_csv(file_path+'C_N_dic.csv',encoding='gbk',index=False)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp=pd.read_excel(other_path+'[New] Macro&Industry_20180613.xlsx',sheetname='指标信息')\n",
    "pp.INDIC_ID=pp.INDIC_ID.astype(str)\n",
    "pp=pp[['INDIC_ID','NAME_CN']]\n",
    "pp.rename(columns={'INDIC_ID':'trains_col'},inplace=True)\n",
    "\n",
    "# dic1=pd.read_csv(other_path+'C_N_dic.csv',encoding='gbk')\n",
    "# dic1\n",
    "\n",
    "# dic2=pd.read_csv(other_path+'C_N_Trans.csv',encoding='gbk')\n",
    "# dic2\n",
    "\n",
    "\n",
    "# pd.merge(dic1,dic2,how='left',on=['E_col']).to_csv(other_path+'CN_dic_3_sheet.csv',index=False,encoding='gbk')\n",
    "\n",
    "\n",
    "# table=pd.read_csv(mid_path+'table_6_file_label_new.csv')\n",
    "# table2=pd.read_csv(other_path+'CN_dic_3_sheet.csv',encoding='gbk')\n",
    "# ttt=DataFrame({'trains_col':table.columns})\n",
    "\n",
    "ww=pd.merge(ttt,table2[['C_col','trains_col']],how='left',on=['trains_col'])\n",
    "\n",
    "pd.merge(ww,pp,how='left',on=['trains_col']).to_csv(file_path+'C_N_6_sheet.csv',index=False,encoding='gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pre_revenue():\n",
    "    A=pd.read_csv(mid_path+'table_6_file_label.csv')\n",
    "    A[A.TICKER_SYMBOL==1].END_DATE.values\n",
    "\n",
    "    tt=['2009-03-31', '2009-06-30', '2009-09-30', '2009-12-31',\n",
    "           '2010-03-31', '2010-06-30', '2010-09-30', '2010-12-31',\n",
    "           '2011-03-31', '2011-06-30', '2011-09-30', '2011-12-31',\n",
    "           '2012-03-31', '2012-06-30', '2012-09-30', '2012-12-31',\n",
    "           '2013-03-31', '2013-06-30', '2013-09-30', '2013-12-31',\n",
    "           '2014-03-31', '2014-06-30', '2014-09-30', '2014-12-31',\n",
    "           '2015-03-31', '2015-06-30', '2015-09-30', '2015-12-31',\n",
    "           '2016-03-31', '2016-06-30', '2016-09-30', '2016-12-31',\n",
    "           '2017-03-31', '2017-06-30', '2017-09-30', '2017-12-31',\n",
    "           '2018-03-31']\n",
    "\n",
    "    w=[]\n",
    "    ww=[]\n",
    "    for i in range(len(tt)-1):\n",
    "        w.append(tt[i])\n",
    "        ww.append(tt[i+1])\n",
    "    W=DataFrame({'END_DATE':w,'END_DATE_PRE':ww})\n",
    "    W\n",
    "    AW=pd.merge(A,W,how='left',on=['END_DATE'])\n",
    "    AW[['TICKER_SYMBOL','END_DATE','END_DATE_PRE','label']]\n",
    "\n",
    "    AW=AW[['TICKER_SYMBOL','END_DATE_PRE','label']]\n",
    "    AW.rename(columns={'END_DATE_PRE':'END_DATE'},inplace=True)\n",
    "    AW\n",
    "\n",
    "\n",
    "    B=pd.read_csv(mid_path+'table_6_file.csv')\n",
    "    ABW=pd.merge(B,AW,how='left',on=['TICKER_SYMBOL','END_DATE'])\n",
    "    ABW\n",
    "\n",
    "    ABW_child=ABW[ABW['END_DATE']=='2018-03-31']\n",
    "    #ABW.replace(0,np.nan,inplace=True)\n",
    "    ABW.dropna(subset=['label'],how='any',inplace=True)\n",
    "    ABW_PLUS=pd.concat([ABW,ABW_child])\n",
    "    ABW_PLUS=ABW_PLUS[['TICKER_SYMBOL','END_DATE','label']]\n",
    "    ABW_PLUS['year']=ABW_PLUS['END_DATE'].apply(lambda x:x[0:4]).astype(int)\n",
    "    ABW_PLUS['month']=ABW_PLUS['END_DATE'].apply(lambda x:x[5:7]).astype(int)\n",
    "    ABW_PLUS.drop(['END_DATE'],axis=1,inplace=True)\n",
    "    \n",
    "    return ABW_PLUS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_revenue.rename(columns={'label':'Pre_revenue'},inplace=True)\n",
    "pre_revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_feature_data():\n",
    "#     '''\n",
    "#     生成初步跑的大表（按要求筛掉部分宏观信息和company_operation 信息）,feature_importance.csv\n",
    "    \n",
    "#     '''\n",
    "\n",
    "##第一步：筛掉ticker和空列，去掉部分宏观指标和龙头所有信息\n",
    "# table=pd.read_csv(mid_path+'table_6_file_label_new.csv')\n",
    "\n",
    "# drop_col=table.columns[379:384].append(table.columns[386:451])\n",
    "# drop_col\n",
    "# table.drop(drop_col,axis=1,inplace=True)\n",
    "\n",
    "# table.drop(['industrySymbol_level_1','industrySymbol_level_2','industrySymbol_level_3',\\\n",
    "#             'indexSymbol_level_1','indexSymbol_level_2','indexSymbol_level_3','classifi_by_14L1',],axis=1,inplace=True)\n",
    "\n",
    "# table=preprocess.select_table(table,class_option,del_col,ticker_option)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# T=pd.get_dummies(table,columns=['FISCAL_PERIOD'])\n",
    "\n",
    "# T.get_dtype_counts()\n",
    "\n",
    "# #添加weight列\n",
    "\n",
    "# weight=pd.read_csv(other_path+'weight.csv')\n",
    "# weight\n",
    "# T=pd.merge(T,weight,how='left',on=['TICKER_SYMBOL'])\n",
    "# T\n",
    "\n",
    "# #添加上季度营业值\n",
    "# pre_revenue=pre_revenue()\n",
    "# pre_revenue.drop_duplicates(inplace=True)\n",
    "# pre_revenue.rename(columns={'label':'Pre_revenue'},inplace=True)\n",
    "# T=pd.merge(T,pre_revenue,how='left',on=['TICKER_SYMBOL','year','month'])\n",
    "# T\n",
    "\n",
    "# #添加当季度营业值\n",
    "# dangji=pd.read_csv(mid_path+'table_6_file_label.csv')\n",
    "# dangji.rename(columns={'label':'Now_revenue'},inplace=True)\n",
    "\n",
    "# merge=pd.merge(T,dangji[['TICKER_SYMBOL','year','month','Now_revenue']],how='left',on=['TICKER_SYMBOL','year','month'])\n",
    "\n",
    "merge\n",
    "\n",
    "#添加date列，'FISCAL_PERIOD'进行onehot\n",
    "# merge['date']=merge['year'].astype(str)+merge['month'].apply(lambda x:\"%02d\"%x)\n",
    "# merge\n",
    "\n",
    "\n",
    "# merge.date.unique()\n",
    "\n",
    "# tt=['200903', '200906', '200909', '200912', '201003', '201006',\n",
    "#        '201009', '201012', '201103', '201106', '201109', '201112',\n",
    "#        '201203', '201206', '201209', '201212', '201303', '201306',\n",
    "#        '201309', '201312', '201403', '201406', '201409', '201412',\n",
    "#        '201503', '201506', '201509', '201512', '201603', '201606',\n",
    "#        '201609', '201612', '201703', '201706', '201709', '201712',\n",
    "#        '201803','201806']\n",
    "\n",
    "\n",
    "# w=[]\n",
    "# ww=[]\n",
    "# for i in range(len(tt)-1):\n",
    "#     w.append(tt[i])\n",
    "#     ww.append(tt[i+1])\n",
    "# W=DataFrame({'era':ww,'date':w})\n",
    "# W\n",
    "# AW=pd.merge(merge,W,how='left',on=['date'])\n",
    "# AW\n",
    "# AW.drop(['date'],axis=1,inplace=True)\n",
    "\n",
    "# AW\n",
    "\n",
    "# header=['era','TICKER_SYMBOL','FISCAL_PERIOD_3', 'FISCAL_PERIOD_6', 'FISCAL_PERIOD_9', 'FISCAL_PERIOD_12']\n",
    "# bottom=['Pre_revenue','Now_revenue','label','weight']\n",
    "# AW=AW[header+list(set(AW.columns)-set(header)-set(bottom))+bottom]\n",
    "\n",
    "##去掉一些列\n",
    "# AW.get_dtype_counts()\n",
    "# AW.drop(['year','month','TICKER_SYMBOL','REVENUE_commom_income'],axis=1,inplace=True)\n",
    "# AW.get_dtype_counts()\n",
    "\n",
    "AW.get_dtype_counts()\n",
    "QQQ=AW[AW.era!='201806']#.to_csv(other_path+'feature_importance_3.csv',index=False)\n",
    "\n",
    "AW[['era','Now_revenue']].to_csv(other_path+'Revenue_ticker.csv',index=False)\n",
    "\n",
    "# merge.get_dtype_counts()\n",
    "# merge[merge.date!='201803'].to_csv(other_path+'feature_importance_2_update.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight():\n",
    "    market_value=pd.read_excel(other_path+'[New] Market Data_20180613.xlsx')\n",
    "    market_value=market_value[market_value.END_DATE_=='2018-05-31']\n",
    "    market_value['weight']=market_value.MARKET_VALUE.apply(lambda x:math.log(max(x/(10**8),2),2))\n",
    "    market_value[['TICKER_SYMBOL','weight']].to_csv(other_path+'weight.csv',index=False)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##探究主表的列类型\n",
    "\n",
    "\n",
    "#A.columns[A.columns.str.contains('REVE')]#列中包含某字符的列名\n",
    "#A.get_dtype_counts() 列的类型计数\n",
    "\n",
    "#A.infer_objects().dtypes#具体到每列的类型\n",
    "#t#[t=='object']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare(df1,df2):\n",
    "#     '''\n",
    "#     比较表间ticker异同\n",
    "    \n",
    "#     '''\n",
    "#     print(len(df1.TICKER_SYMBOL.unique()))\n",
    "#     print(len(df2.TICKER_SYMBOL.unique()))\n",
    "#     print(set(df1.TICKER_SYMBOL.unique())-set(df2.TICKER_SYMBOL.unique()))\n",
    "#     print(set(df2.TICKER_SYMBOL.unique())-set(df1.TICKER_SYMBOL.unique()))\n",
    "    \n",
    "    \n",
    "# compare(balance_sheet_Securities_old,income_sheet_Securities_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ##1.market data 检查value 和申万知否一致\n",
    "    \n",
    "#market_data=pd.read_excel(raw_data_path+'[New] Market Data_20180613.xlsx')\n",
    "#market_data.TICKER_SYMBOL.unique().shape#(3675,)\n",
    "# test_180330=market_data[market_data['END_DATE_']=='2018-03-30'][['TICKER_SYMBOL','MARKET_VALUE']]\n",
    "# check_180830=pd.read_csv(file_path+'Uer_market_2018330.csv')\n",
    "# check_180830=check_180830[['ticker','negMarketValue','marketValue']].rename(columns={'ticker':'TICKER_SYMBOL'})\n",
    "# pd.merge(test_180330,check_180830,how='left',on=['TICKER_SYMBOL']).to_csv(file_path+'check_market_values.csv',index=False)\n",
    "#================================================================\n",
    "    ##2.从三表中读取12表\n",
    "\n",
    "# balance_sheet_Bank,balance_sheet_Securities, balance_sheet_Insurance,balance_sheet_GB,\\\n",
    "# cashFlow_sheet_Bank,cashFlow_sheet_Securities,cashFlow_sheet_Insurance,cashFlow_sheet_GB,\\\n",
    "# income_sheet_Bank,income_sheet_Securities,income_sheet_Insurance,income_sheet_GB=utils.loadingFile(0)\n",
    "\n",
    "#================================================================\n",
    "    ##3.将12个表写到csv中，方便今后读取\n",
    "\n",
    "# file_name=['balance_sheet_Bank','balance_sheet_Securities', 'balance_sheet_Insurance','balance_sheet_GB',\\\n",
    "# 'cashFlow_sheet_Bank','cashFlow_sheet_Securities','cashFlow_sheet_Insurance','cashFlow_sheet_GB',\\\n",
    "# 'income_sheet_Bank','income_sheet_Securities','income_sheet_Insurance','income_sheet_GB']\n",
    "\n",
    "# file_df=[balance_sheet_Bank,balance_sheet_Securities, balance_sheet_Insurance,balance_sheet_GB,\\\n",
    "# cashFlow_sheet_Bank,cashFlow_sheet_Securities,cashFlow_sheet_Insurance,cashFlow_sheet_GB,\\\n",
    "# income_sheet_Bank,income_sheet_Securities,income_sheet_Insurance,income_sheet_GB]\n",
    "\n",
    "# for df,name in zip(file_df,file_name):\n",
    "#     df.to_csv(file_path+'files/financial_data_new/%s.csv'%name)\n",
    "#============================================================================\n",
    "    ## 4.在csv文件中补全00000\n",
    "\n",
    "# A=pd.read_csv(file_path+'sumbit_5_31.csv',encoding='gbk')\n",
    "# A.TICKER_SYMBOL=A.TICKER_SYMBOL.apply(lambda x:'\\t%06d'%x)\n",
    "# A.to_csv(file_path+'523.csv',index=False)\n",
    "\n",
    "# A=pd.read_csv(file_path+'sumbit_5_31.csv',encoding='gbk')\n",
    "# A.TICKER_SYMBOL=A.TICKER_SYMBOL.apply(lambda x:'%06d'%x)\n",
    "# #A.to_csv(file_path+'523.csv',index=False)\n",
    "# for a in A.TICKER_SYMBOL:\n",
    "#     print(a)\n",
    "\n",
    "#==============================================================\n",
    "    ## 5.补全下载数据的ticker\n",
    "# file_df=pd.read_csv(file_path+'cashflow_daily_all.csv')\n",
    "# #file_df=pd.read_hdf(file_path+'CAI.h5',key='data')\n",
    "# ticker_df=pd.read_csv(file_path+'ticker_list_new.csv')\n",
    "# file_tickers=file_df.ticker.unique()\n",
    "# ticker_tickers=ticker_df['0'].values\n",
    "# dfii_list=[x for x in ticker_tickers if x not in file_tickers]\n",
    "# DataFrame(dfii_list).to_csv(file_path+'DIFF-个股资金日流向.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#===================================================================================    \n",
    "    ##三表ticker检查\n",
    "'''\n",
    "compared with 529data:add 5 more tickers\n",
    " array([300747], dtype=int64),\n",
    " array([601066], dtype=int64),\n",
    " array([603650], dtype=int64),\n",
    " array([603666], dtype=int64),\n",
    " array([603693], dtype=int64)]\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Balance=[balance_sheet_Bank_old,balance_sheet_Securities_old,balance_sheet_Insurance_old,balance_sheet_GB_old]\n",
    "# del balance_sheet_Bank_old,balance_sheet_Securities_old,balance_sheet_Insurance_old,balance_sheet_GB_old\n",
    "# Balance\n",
    "\n",
    "# Balance_df=pd.concat(Balance)\n",
    "# Balance_df.TICKER_SYMBOL.unique()#3551\n",
    "\n",
    "# Cash=[cashFlow_sheet_Bank_old,cashFlow_sheet_Securities_old,cashFlow_sheet_Insurance_old,cashFlow_sheet_GB_old]\n",
    "# del cashFlow_sheet_Bank_old,cashFlow_sheet_Securities_old,cashFlow_sheet_Insurance_old,cashFlow_sheet_GB_old\n",
    "# Cash\n",
    "\n",
    "# Cash_df=pd.concat(Cash)\n",
    "# Cash_df.TICKER_SYMBOL.unique()#3551\n",
    "\n",
    "# Income=[income_sheet_Bank_old,income_sheet_Securities_old,income_sheet_Insurance_old,income_sheet_GB_old]\n",
    "# del income_sheet_Bank_old,income_sheet_Securities_old,income_sheet_Insurance_old,income_sheet_GB_old\n",
    "# Income\n",
    "\n",
    "# Income_df=pd.concat(Income)\n",
    "# Income_df.TICKER_SYMBOL.unique()#3551\n",
    "\n",
    "\n",
    "\n",
    "# print('the shape of Balance_df is',Balance_df.TICKER_SYMBOL.unique().shape)#(3556,)\n",
    "# print('the shape of Cash_df is',Cash_df.TICKER_SYMBOL.unique().shape)#(3556,)\n",
    "# print('the shape of Income_df is',Income_df.TICKER_SYMBOL.unique().shape)#()(3556,)\n",
    "\n",
    "\n",
    "# tiker_list=pd.read_csv(file_path+'ticker_list.csv')\n",
    "# tiker_list['0'].unique()\n",
    "\n",
    "\n",
    "# #比较三大表中ticker是否相同\n",
    "# A1=set(Balance_df.TICKER_SYMBOL.unique())\n",
    "# A2=set(Cash_df.TICKER_SYMBOL.unique())\n",
    "# A3=set(Income_df.TICKER_SYMBOL.unique())\n",
    "\n",
    "# print('A1==A2',A1==A2)\n",
    "# print('A1==A3',A1==A3)\n",
    "# print('A2==A3',A2==A3)\n",
    "\n",
    "#DataFrame(sorted(Balance_df.TICKER_SYMBOL.unique())).to_csv(file_path+'ticker_list_new_613.csv',index=False)\n",
    "\n",
    "#===============================================================   \n",
    "    ##检查529和613文件中ticker异同\n",
    "# ticker_529=pd.read_csv(file_path+'ticker_list_new.csv')\n",
    "# ticker_613=pd.read_csv(file_path+'ticker_list_new_613.csv')\n",
    "# [x for x in ticker_613.values if x not in ticker_529.values]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
