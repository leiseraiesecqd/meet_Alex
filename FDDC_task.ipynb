{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utils.ipynb\n",
      "importing Jupyter notebook from preprocess.ipynb\n",
      "importing Jupyter notebook from merge.ipynb\n",
      "----------------------------------------------------\n",
      "Starting preprocessing....\n",
      "\n",
      "=======================================================\n",
      "loading file...\n",
      "reading file:balance_sheet_Bank_new.csv\n",
      "reading file:balance_sheet_GB_new.csv\n",
      "reading file:balance_sheet_Insurance_new.csv\n",
      "reading file:balance_sheet_Securities_new.csv\n",
      "reading file:cashFlow_sheet_Bank_new.csv\n",
      "reading file:cashFlow_sheet_GB_new.csv\n",
      "reading file:cashFlow_sheet_Insurance_new.csv\n",
      "reading file:cashFlow_sheet_Securities_new.csv\n",
      "reading file:income_sheet_Bank_new.csv\n",
      "reading file:income_sheet_GB_new.csv\n",
      "reading file:income_sheet_Insurance_new.csv\n",
      "reading file:income_sheet_Securities_new.csv\n",
      "=======================================================\n",
      "del some cols...\n",
      "del col of df\n",
      "del col of df\n",
      "del col of df\n",
      "del col of df\n",
      "del col of df\n",
      "del col of df\n",
      "del col of df\n",
      "del col of df\n",
      "del col of df\n",
      "del col of df\n",
      "del col of df\n",
      "del col of df\n",
      "single sheet drop_duplicates...\n",
      "=======================================================\n",
      "preprosessing the normal ticker...\n",
      "merging Balance sheet.....\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mr.handsome\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py:3778: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging CashFlow sheet.....\n",
      "\n",
      "merging Income sheet.....\n",
      "\n",
      "merging final sheet.....\n",
      "\n",
      "preprosessing the normal ticker done!!!...\n",
      "\n",
      "=======================================================\n",
      "preprosessing the special ticker...\n",
      "merging Balance sheet.....\n",
      "\n",
      "merging CashFlow sheet.....\n",
      "\n",
      "merging Income sheet.....\n",
      "\n",
      "merging final sheet.....\n",
      "\n",
      "preprosessing the special ticker done!!!...\n",
      "=======================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f836f2165b27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[0moht_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TICKER_SYMBOL'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'FISCAL_PERIOD'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'industrySymbol_level_1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'industrySymbol_level_2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'industrySymbol_level_1'\u001b[0m\u001b[1;33m,\u001b[0m        \u001b[1;34m'indexSymbol_level_1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'indexSymbol_level_2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'indexSymbol_level_3'\u001b[0m\u001b[1;33m,\u001b[0m        \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m \u001b[0mmerge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdel_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;31m# merge.generate_data_base()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;31m# merge.merge_3file()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\a_FDDC\\merge.ipynb\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(del_list)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mr.handsome\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2677\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2678\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2679\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2680\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2681\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mr.handsome\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2722\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2723\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2724\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2726\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mr.handsome\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_take\u001b[1;34m(self, indices, axis, is_copy)\u001b[0m\n\u001b[0;32m   2784\u001b[0m         new_data = self._data.take(indices,\n\u001b[0;32m   2785\u001b[0m                                    \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2786\u001b[1;33m                                    verify=True)\n\u001b[0m\u001b[0;32m   2787\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mr.handsome\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[0;32m   4537\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4538\u001b[0m         return self.reindex_indexer(new_axis=new_labels, indexer=indexer,\n\u001b[1;32m-> 4539\u001b[1;33m                                     axis=axis, allow_dups=True)\n\u001b[0m\u001b[0;32m   4540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4541\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mr.handsome\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[0;32m   4419\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4420\u001b[0m             new_blocks = self._slice_take_blocks_ax0(indexer,\n\u001b[1;32m-> 4421\u001b[1;33m                                                      fill_tuple=(fill_value,))\n\u001b[0m\u001b[0;32m   4422\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4423\u001b[0m             new_blocks = [blk.take_nd(indexer, axis=axis, fill_tuple=(\n",
      "\u001b[1;32mc:\\users\\mr.handsome\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_tuple)\u001b[0m\n\u001b[0;32m   4499\u001b[0m                     blocks.append(blk.take_nd(blklocs[mgr_locs.indexer],\n\u001b[0;32m   4500\u001b[0m                                               \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4501\u001b[1;33m                                               fill_tuple=None))\n\u001b[0m\u001b[0;32m   4502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4503\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mr.handsome\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_tuple)\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[0mfill_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m             new_values = algos.take_nd(values, indexer, axis=axis,\n\u001b[1;32m-> 1254\u001b[1;33m                                        allow_fill=False)\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m             \u001b[0mfill_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfill_tuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import Ipynb_importer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import utils,preprocess,merge\n",
    "import time\n",
    "import datetime\n",
    "import os,gc\n",
    "file_path='D:/360WiFi/'\n",
    "raw_data_path='D:/360WiFi/FDDC_financial_data_20180613/'\n",
    "split_file_path='D:/360WiFi/files/financial_data_new/'\n",
    "input_path='D:/360WiFi/files/input/'\n",
    "mid_path='D:/360WiFi/files/mid_file/'\n",
    "other_path='D:/360WiFi/files/other_file/'\n",
    "\n",
    "\n",
    "##------------------合表前，初筛列开关--------------------------------------------------------------\n",
    "#资产负债表\n",
    "del_col_Balance_Bank=['AE,AA,LE,LA,SEE,SEA,OTH_EFFECT_SE,OTH_EFFECT_SA,LEE,LEA']\n",
    "del_col_Balance_GB=['CAE,CAA,NCAE,NCAA,AE,AA,CLE,CLA,NCLE,NCLA,LE,LA,SEE,SEA,OTH_EFFECT_SE,OTH_EFFECT_SA,LEE,LEA']\n",
    "del_col_Balance_Insurance=['AE,AA,LE,LA,SEE,SEA,OTH_EFFECT_SE,OTH_EFFECT_SA,LEE,LEA']\n",
    "del_col_Balance_Securities=['AE,AA,LE,LA,SEE,SEA,OTH_EFFECT_SE,OTH_EFFECT_SA,LEE,LEA']\n",
    "\n",
    "#现金流量表\n",
    "del_col_cashFlow_Bank=['SPEC_OCIF,AOCIF,SPEC_OCOF,AOCOF,ANOCF,SPEC_ICIF,AICIF,SPEC_ICOF,AICOF,ANICF,SPEC_FCIF,AFCIF,SPEC_FCOF,AFCOF,ANFCF,OTH_EFFECT_CE,ACE,OTH_EFFECT_CEI,ACEI']\n",
    "del_col_cashFlow_GB=['SPEC_OCIF,AOCIF,SPEC_OCOF,AOCOF,ANOCF,SPEC_ICIF,AICIF,SPEC_ICOF,AICOF,ANICF,SPEC_FCIF,AFCIF,SPEC_FCOF,AFCOF,ANFCF,OTH_EFFECT_CE,ACE,OTH_EFFECT_CEI,ACEI']\n",
    "del_col_cashFlow_Insurance=['SPEC_OCIF,AOCIF,SPEC_OCOF,AOCOF,ANOCF,SPEC_ICIF,AICIF,SPEC_ICOF,AICOF,ANICF,SPEC_FCIF,AFCIF,SPEC_FCOF,AFCOF,ANFCF,OTH_EFFECT_CE,ACE,OTH_EFFECT_CEI,ACEI']\n",
    "del_col_cashFlow_Securities=['SPEC_OCIF,AOCIF,SPEC_OCOF,AOCOF,ANOCF,SPEC_ICIF,AICIF,SPEC_ICOF,AICOF,ANICF,SPEC_FCIF,AFCIF,SPEC_FCOF,AFCOF,ANFCF,OTH_EFFECT_CE,ACE,OTH_EFFECT_CEI,ACEI']\n",
    "\n",
    "\n",
    "#利润表\n",
    "del_col_Income_Bank=['SPEC_OR,AOR,SPEC_OC,AOC,OTH_EFFECT_OP,AE_EFFECT_OP,OTH_EFFECT_TP,AE_EFFECT_TP,OTH_EFFECT_NP,AE_EFFECT_NP,OTH_EFFECT_NPP,AE_EFFECT_NPP,OTH_EFFECT_CI,AE_EFFECT_CI,OTH_EFFECT_PCI,AE_EFFECT_PCI']\n",
    "del_col_Income_GB=['SPEC_TOR,ATOR,SPEC_TOC,ATOC,OTH_EFFECT_OP,AE_EFFECT_OP,OTH_EFFECT_TP,AE_EFFECT_TP,OTH_EFFECT_NP,AE_EFFECT_NP,OTH_EFFECT_NPP,AE_EFFECT_NPP,OTH_EFFECT_CI,AE_EFFECT_CI,OTH_EFFECT_PCI,AE_EFFECT_PCI']\n",
    "del_col_Income_Insurance=['SPEC_OR,AOR,SPEC_OC,AOC,OTH_EFFECT_OP,AE_EFFECT_OP,OTH_EFFECT_TP,AE_EFFECT_TP,OTH_EFFECT_NP,AE_EFFECT_NP,OTH_EFFECT_NPP,AE_EFFECT_NPP,OTH_EFFECT_CI,AE_EFFECT_CI,OTH_EFFECT_PCI,AE_EFFECT_PCI']\n",
    "del_col_Income_Securities=['SPEC_OR,AOR,SPEC_OC,AOC,OTH_EFFECT_OP,AE_EFFECT_OP,OTH_EFFECT_TP,AE_EFFECT_TP,OTH_EFFECT_NP,AE_EFFECT_NP,OTH_EFFECT_NPP,AE_EFFECT_NPP,OTH_EFFECT_CI,AE_EFFECT_CI,OTH_EFFECT_PCI,AE_EFFECT_PCI']\n",
    "\n",
    "\n",
    "\n",
    "del_list=[del_col_Balance_Bank,del_col_Balance_GB,del_col_Balance_Insurance,del_col_Balance_Securities,\\\n",
    "         del_col_cashFlow_Bank,del_col_cashFlow_GB,del_col_cashFlow_Insurance,del_col_cashFlow_Securities,\\\n",
    "        del_col_Income_Bank,del_col_Income_GB,del_col_Income_Insurance,del_col_Income_Securities]\n",
    "\n",
    "\n",
    "##------------------合表后，删列/删行开关-------------------------------------\n",
    "\n",
    "class_option='classifi_by_14L1'\n",
    "\n",
    "\n",
    "##'END_DATE'会在函数中删除，'REVENUE_commom_income'作为特征,暂不删除\n",
    "\n",
    "del_col=['PARTY_ID','EXCHANGE_CD', 'PUBLISH_DATE','END_DATE_REP',\\\n",
    "          'REPORT_TYPE','MERGED_FLAG', 'category',\\\n",
    "        'industryName_level_1','industryName_level_2','industryName_level_3','isNew','Mid_L2','Mid_L3',\\\n",
    "         'industryID_level_1','industryID_level_2','industryID_level_3',\\\n",
    "         'PUBLISH_DATE_y','PUBLISH_DATE_x','END_DATE_REP_x','END_DATE_REP_y'\n",
    "        ]\n",
    "\n",
    "\n",
    "'''\n",
    "ticker_option的value中：第一项是起始时间，第二项是结束时间，第三项是要删的表，有四种选项['income','balance','cash','all']\n",
    "\n",
    "'''\n",
    "ticker_option={'601360':['2009-03-31','2018-03-31','all'],\n",
    "               '600048':['2017-12-31','2018-03-31','all'],\n",
    "               '600409':['2017-12-31','2018-03-31','all'],\n",
    "               '600693':['2017-12-31','2018-03-31','all'],\n",
    "               '600939':['2017-12-31','2018-03-31','all'],\n",
    "               '600399':['2017-12-31','2018-03-31','all'],\n",
    "               '600610':['2017-12-31','2018-03-31','all'],\n",
    "               '600094':['2009-03-31','2009-09-30','all'],\n",
    "              }\n",
    "\n",
    "##--------------one_hot开关------------------------------------\n",
    "\n",
    "oht_col=['TICKER_SYMBOL','FISCAL_PERIOD','industrySymbol_level_1','industrySymbol_level_2','industrySymbol_level_1',\\\n",
    "        'indexSymbol_level_1','indexSymbol_level_2','indexSymbol_level_3',\\\n",
    "        ]\n",
    "# merge.preprocess(del_list)\n",
    "# merge.generate_data_base()\n",
    "# merge.merge_3file()\n",
    "# preprocess.select_table(tabel,class_option,del_col,ticker_option)\n",
    "# preprocess.make_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "生成初步跑的大表（官方数据）\n",
    "\n",
    "'''\n",
    "\n",
    "# table=pd.read_csv(mid_path+'feature_importance_2.csv')\n",
    "# table.get_dtype_counts()\n",
    "# t=table.infer_objects().dtypes\n",
    "# t[t=='int64']\n",
    "\n",
    "# table['date']=table['year'].astype(str)+table['month'].apply(lambda x:\"%02d\"%x)\n",
    "# table[['date','year','month']]\n",
    "\n",
    "# T=pd.get_dummies(table,columns=['FISCAL_PERIOD'])\n",
    "# T.get_dtype_counts()\n",
    "# header=['date','FISCAL_PERIOD_3', 'FISCAL_PERIOD_6', 'FISCAL_PERIOD_9', 'FISCAL_PERIOD_12']\n",
    "# T=T[header+list((set(T.columns)-set(header)))]\n",
    "# T\n",
    "# T.get_dtype_counts()\n",
    "\n",
    "# dangji=pd.read_csv(mid_path+'table_6_file_label.csv')\n",
    "# dangji.rename(columns={'label':'Revenue_now'},inplace=True)\n",
    "\n",
    "# merge=pd.merge(T,dangji[['TICKER_SYMBOL','year','month','Revenue_now']],how='left',on=['TICKER_SYMBOL','year','month'])\n",
    "# rr=merge[['TICKER_SYMBOL','year','month','Revenue_now','label']]\n",
    "# rr[rr.year!=2018].to_csv(other_path+'check_revenue_prerevenue.csv',index=False)\n",
    "\n",
    "#merge.drop(['year','month','TICKER_SYMBOL','REVENUE_commom_income'],axis=1,inplace=True)\n",
    "# table\n",
    "#merge.get_dtype_counts()\n",
    "#merge[merge.date!='201803'].to_csv(other_path+'feature_importance_2_update.csv',index=False)\n",
    "preprocess.make_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PARTY_ID'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table=pd.read_csv(mid_path+'table_6_file_label_new.csv')\n",
    "columns=table.columns\n",
    "columns[columns.str.contains('PARTY_ID')][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mr.handsome\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\excel.py:329: FutureWarning: The `sheetname` keyword is deprecated, use `sheet_name` instead\n",
      "  **kwds)\n"
     ]
    }
   ],
   "source": [
    "def C_N_dic():\n",
    "\n",
    "\n",
    "    sheetname=['bs_indu','bs_bank','bs_secu','bs_insu','is_indu','is_bank','is_secu','is_insu','cf_indu','cf_bank','cf_secu','cf_insu']\n",
    "\n",
    "    E_col=[]\n",
    "    C_col=[]\n",
    "    source=[]\n",
    "    for name in sheetname:\n",
    "        C_N=pd.read_excel(other_path+'Data Dictionary_5.29.xlsx',sheetname=name)\n",
    "        t=C_N.drop([0,1,2,3])\n",
    "        E_col.extend(t.iloc[:,0].values)\n",
    "        C_col.extend(t.iloc[:,2].values)\n",
    "        source.extend([name for i in range(len(t.iloc[:,2].values))])\n",
    "\n",
    "    C_N=DataFrame({'E_col':E_col,'C_col':C_col,'source':source})\n",
    "    C_N.drop_duplicates(subset=['E_col'],inplace=True)\n",
    "\n",
    "    ##注意一下COGS\n",
    "\n",
    "    #选取第几列\n",
    "    #t.iloc[:,[0,2]]\n",
    "\n",
    "    C_N.to_csv(file_path+'C_N_dic.csv',encoding='gbk',index=False)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mr.handsome\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\io\\excel.py:329: FutureWarning: The `sheetname` keyword is deprecated, use `sheet_name` instead\n",
      "  **kwds)\n"
     ]
    }
   ],
   "source": [
    "pp=pd.read_excel(other_path+'[New] Macro&Industry_20180613.xlsx',sheetname='指标信息')\n",
    "pp.INDIC_ID=pp.INDIC_ID.astype(str)\n",
    "pp=pp[['INDIC_ID','NAME_CN']]\n",
    "pp.rename(columns={'INDIC_ID':'trains_col'},inplace=True)\n",
    "\n",
    "# dic1=pd.read_csv(other_path+'C_N_dic.csv',encoding='gbk')\n",
    "# dic1\n",
    "\n",
    "# dic2=pd.read_csv(other_path+'C_N_Trans.csv',encoding='gbk')\n",
    "# dic2\n",
    "\n",
    "\n",
    "# pd.merge(dic1,dic2,how='left',on=['E_col']).to_csv(other_path+'CN_dic_3_sheet.csv',index=False,encoding='gbk')\n",
    "\n",
    "\n",
    "# table=pd.read_csv(mid_path+'table_6_file_label_new.csv')\n",
    "# table2=pd.read_csv(other_path+'CN_dic_3_sheet.csv',encoding='gbk')\n",
    "# ttt=DataFrame({'trains_col':table.columns})\n",
    "\n",
    "ww=pd.merge(ttt,table2[['C_col','trains_col']],how='left',on=['trains_col'])\n",
    "\n",
    "pd.merge(ww,pp,how='left',on=['trains_col']).to_csv(file_path+'C_N_6_sheet.csv',index=False,encoding='gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_feature_data():\n",
    "    '''\n",
    "    生成初步跑的大表（按要求筛掉部分宏观信息和company_operation 信息）,feature_importance.csv\n",
    "    \n",
    "    '''\n",
    "    table=pd.read_csv(mid_path+'table_6_file_label_new.csv')\n",
    "    table=preprocess.select_table(table,class_option,del_col,ticker_option)\n",
    "\n",
    "    table.drop(table.columns[355:427],axis=1,inplace=True)\n",
    "    table.drop(['industrySymbol_level_1','industrySymbol_level_2','industrySymbol_level_3',\\\n",
    "                'indexSymbol_level_1','indexSymbol_level_2','indexSymbol_level_3','classifi_by_14L1',],axis=1,inplace=True)\n",
    "    table.to_csv(other_path+'feature_importance.csv',index=False)\n",
    "    print('the feature_importance.csv  shape:',table.shape)\n",
    "    return \n",
    "\n",
    "check_revenue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##探究主表的列类型\n",
    "\n",
    "\n",
    "#A.columns[A.columns.str.contains('REVE')]#列中包含某字符的列名\n",
    "#A.get_dtype_counts() 列的类型计数\n",
    "\n",
    "#A.infer_objects().dtypes#具体到每列的类型\n",
    "#t#[t=='object']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare(df1,df2):\n",
    "#     '''\n",
    "#     比较表间ticker异同\n",
    "    \n",
    "#     '''\n",
    "#     print(len(df1.TICKER_SYMBOL.unique()))\n",
    "#     print(len(df2.TICKER_SYMBOL.unique()))\n",
    "#     print(set(df1.TICKER_SYMBOL.unique())-set(df2.TICKER_SYMBOL.unique()))\n",
    "#     print(set(df2.TICKER_SYMBOL.unique())-set(df1.TICKER_SYMBOL.unique()))\n",
    "    \n",
    "    \n",
    "# compare(balance_sheet_Securities_old,income_sheet_Securities_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ##1.market data 检查value 和申万知否一致\n",
    "    \n",
    "#market_data=pd.read_excel(raw_data_path+'[New] Market Data_20180613.xlsx')\n",
    "#market_data.TICKER_SYMBOL.unique().shape#(3675,)\n",
    "# test_180330=market_data[market_data['END_DATE_']=='2018-03-30'][['TICKER_SYMBOL','MARKET_VALUE']]\n",
    "# check_180830=pd.read_csv(file_path+'Uer_market_2018330.csv')\n",
    "# check_180830=check_180830[['ticker','negMarketValue','marketValue']].rename(columns={'ticker':'TICKER_SYMBOL'})\n",
    "# pd.merge(test_180330,check_180830,how='left',on=['TICKER_SYMBOL']).to_csv(file_path+'check_market_values.csv',index=False)\n",
    "#================================================================\n",
    "    ##2.从三表中读取12表\n",
    "\n",
    "# balance_sheet_Bank,balance_sheet_Securities, balance_sheet_Insurance,balance_sheet_GB,\\\n",
    "# cashFlow_sheet_Bank,cashFlow_sheet_Securities,cashFlow_sheet_Insurance,cashFlow_sheet_GB,\\\n",
    "# income_sheet_Bank,income_sheet_Securities,income_sheet_Insurance,income_sheet_GB=utils.loadingFile(0)\n",
    "\n",
    "#================================================================\n",
    "    ##3.将12个表写到csv中，方便今后读取\n",
    "\n",
    "# file_name=['balance_sheet_Bank','balance_sheet_Securities', 'balance_sheet_Insurance','balance_sheet_GB',\\\n",
    "# 'cashFlow_sheet_Bank','cashFlow_sheet_Securities','cashFlow_sheet_Insurance','cashFlow_sheet_GB',\\\n",
    "# 'income_sheet_Bank','income_sheet_Securities','income_sheet_Insurance','income_sheet_GB']\n",
    "\n",
    "# file_df=[balance_sheet_Bank,balance_sheet_Securities, balance_sheet_Insurance,balance_sheet_GB,\\\n",
    "# cashFlow_sheet_Bank,cashFlow_sheet_Securities,cashFlow_sheet_Insurance,cashFlow_sheet_GB,\\\n",
    "# income_sheet_Bank,income_sheet_Securities,income_sheet_Insurance,income_sheet_GB]\n",
    "\n",
    "# for df,name in zip(file_df,file_name):\n",
    "#     df.to_csv(file_path+'files/financial_data_new/%s.csv'%name)\n",
    "#============================================================================\n",
    "    ## 4.在csv文件中补全00000\n",
    "\n",
    "# A=pd.read_csv(file_path+'sumbit_5_31.csv',encoding='gbk')\n",
    "# A.TICKER_SYMBOL=A.TICKER_SYMBOL.apply(lambda x:'\\t%06d'%x)\n",
    "# A.to_csv(file_path+'523.csv',index=False)\n",
    "\n",
    "# A=pd.read_csv(file_path+'sumbit_5_31.csv',encoding='gbk')\n",
    "# A.TICKER_SYMBOL=A.TICKER_SYMBOL.apply(lambda x:'%06d'%x)\n",
    "# #A.to_csv(file_path+'523.csv',index=False)\n",
    "# for a in A.TICKER_SYMBOL:\n",
    "#     print(a)\n",
    "\n",
    "#==============================================================\n",
    "    ## 5.补全下载数据的ticker\n",
    "# file_df=pd.read_csv(file_path+'cashflow_daily_all.csv')\n",
    "# #file_df=pd.read_hdf(file_path+'CAI.h5',key='data')\n",
    "# ticker_df=pd.read_csv(file_path+'ticker_list_new.csv')\n",
    "# file_tickers=file_df.ticker.unique()\n",
    "# ticker_tickers=ticker_df['0'].values\n",
    "# dfii_list=[x for x in ticker_tickers if x not in file_tickers]\n",
    "# DataFrame(dfii_list).to_csv(file_path+'DIFF-个股资金日流向.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#===================================================================================    \n",
    "    ##三表ticker检查\n",
    "'''\n",
    "compared with 529data:add 5 more tickers\n",
    " array([300747], dtype=int64),\n",
    " array([601066], dtype=int64),\n",
    " array([603650], dtype=int64),\n",
    " array([603666], dtype=int64),\n",
    " array([603693], dtype=int64)]\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Balance=[balance_sheet_Bank_old,balance_sheet_Securities_old,balance_sheet_Insurance_old,balance_sheet_GB_old]\n",
    "# del balance_sheet_Bank_old,balance_sheet_Securities_old,balance_sheet_Insurance_old,balance_sheet_GB_old\n",
    "# Balance\n",
    "\n",
    "# Balance_df=pd.concat(Balance)\n",
    "# Balance_df.TICKER_SYMBOL.unique()#3551\n",
    "\n",
    "# Cash=[cashFlow_sheet_Bank_old,cashFlow_sheet_Securities_old,cashFlow_sheet_Insurance_old,cashFlow_sheet_GB_old]\n",
    "# del cashFlow_sheet_Bank_old,cashFlow_sheet_Securities_old,cashFlow_sheet_Insurance_old,cashFlow_sheet_GB_old\n",
    "# Cash\n",
    "\n",
    "# Cash_df=pd.concat(Cash)\n",
    "# Cash_df.TICKER_SYMBOL.unique()#3551\n",
    "\n",
    "# Income=[income_sheet_Bank_old,income_sheet_Securities_old,income_sheet_Insurance_old,income_sheet_GB_old]\n",
    "# del income_sheet_Bank_old,income_sheet_Securities_old,income_sheet_Insurance_old,income_sheet_GB_old\n",
    "# Income\n",
    "\n",
    "# Income_df=pd.concat(Income)\n",
    "# Income_df.TICKER_SYMBOL.unique()#3551\n",
    "\n",
    "\n",
    "\n",
    "# print('the shape of Balance_df is',Balance_df.TICKER_SYMBOL.unique().shape)#(3556,)\n",
    "# print('the shape of Cash_df is',Cash_df.TICKER_SYMBOL.unique().shape)#(3556,)\n",
    "# print('the shape of Income_df is',Income_df.TICKER_SYMBOL.unique().shape)#()(3556,)\n",
    "\n",
    "\n",
    "# tiker_list=pd.read_csv(file_path+'ticker_list.csv')\n",
    "# tiker_list['0'].unique()\n",
    "\n",
    "\n",
    "# #比较三大表中ticker是否相同\n",
    "# A1=set(Balance_df.TICKER_SYMBOL.unique())\n",
    "# A2=set(Cash_df.TICKER_SYMBOL.unique())\n",
    "# A3=set(Income_df.TICKER_SYMBOL.unique())\n",
    "\n",
    "# print('A1==A2',A1==A2)\n",
    "# print('A1==A3',A1==A3)\n",
    "# print('A2==A3',A2==A3)\n",
    "\n",
    "#DataFrame(sorted(Balance_df.TICKER_SYMBOL.unique())).to_csv(file_path+'ticker_list_new_613.csv',index=False)\n",
    "\n",
    "#===============================================================   \n",
    "    ##检查529和613文件中ticker异同\n",
    "# ticker_529=pd.read_csv(file_path+'ticker_list_new.csv')\n",
    "# ticker_613=pd.read_csv(file_path+'ticker_list_new_613.csv')\n",
    "# [x for x in ticker_613.values if x not in ticker_529.values]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
