{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import time\n",
    "import datetime\n",
    "import os,gc\n",
    "file_path='D:/360WiFi/'\n",
    "raw_data_path='D:/360WiFi/FDDC_financial_data_20180613/'\n",
    "split_file_path='D:/360WiFi/files/financial_data_new/'\n",
    "'''\n",
    "Function List:\n",
    "\n",
    "    loadingFile:读取原始文件\n",
    "    loading_12_file:读取12表\n",
    "    date_time：将表中三日期datatime化,并排序\n",
    "    date_time_index：date_time排序的时候加上INDEX\n",
    "    merge_3sheet：合成三大表\n",
    "    ticker_3sheet_info：检查上市后ticker是否有三表缺失情况\n",
    "    check_uniqueness：检查三DATE联立后是否有冲突数据\n",
    "    fill_nan_PIT:将12表的point in time 最新数据缺失值补全\n",
    "    \n",
    "    preprocess:合三表（涉及到）preprocess_common，preprocess_normal，preprocess_special，\n",
    "                              其中preprocess_common又涉及到loading_inputs\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def loadingFile(whether_code):\n",
    "    '''\n",
    "    whether_code=1,替换columns\n",
    "    whether_code=0，不替换columns\n",
    "    \n",
    "    \n",
    "    return 12 original files \n",
    "    use time: 25mins\n",
    "    '''\n",
    "    #------处理 BalanceSheet---------------------\n",
    "    \n",
    "    start_time=time.time()\n",
    "    balance_sheet_Bank = pd.read_excel(raw_data_path+'Balance Sheet.xls',sheetname='Bank')\n",
    "    balance_sheet_Securities = pd.read_excel(raw_data_path+'Balance Sheet.xls',sheetname='Securities')\n",
    "    balance_sheet_Insurance = pd.read_excel(raw_data_path+'Balance Sheet.xls',sheetname='Insurance')\n",
    "    balance_sheet_GB = pd.read_excel(raw_data_path+'Balance Sheet.xls',sheetname='General Business')\n",
    "\n",
    "    print('Reading Balance sheet use time:{}\\n'.format(time.time()-start_time))\n",
    "\n",
    "\n",
    "    #处理---------- Cash Flow Statement--------------------\n",
    "    \n",
    "    start_time=time.time()\n",
    "    cashFlow_sheet_Bank = pd.read_excel(raw_data_path+'Cash Flow Statement.xls',sheetname='Bank')\n",
    "    cashFlow_sheet_Securities = pd.read_excel(raw_data_path+'Cash Flow Statement.xls',sheetname='Securities')\n",
    "    cashFlow_sheet_Insurance = pd.read_excel(raw_data_path+'Cash Flow Statement.xls',sheetname='Insurance')\n",
    "    cashFlow_sheet_GB = pd.read_excel(raw_data_path+'Cash Flow Statement.xls',sheetname='General Business')\n",
    "    print('Reading Cash Flow sheet use time:{}\\n'.format(time.time()-start_time))\n",
    "    cashFlow_sheet_GB\n",
    "\n",
    "\n",
    "    #处理 ----------Income Statement------------------------\n",
    "\n",
    "    start_time=time.time()\n",
    "    income_sheet_Bank = pd.read_excel(raw_data_path+'Income Statement.xls',sheetname='Bank')\n",
    "    income_sheet_Securities = pd.read_excel(raw_data_path+'Income Statement.xls',sheetname='Securities')\n",
    "    income_sheet_Insurance = pd.read_excel(raw_data_path+'Income Statement.xls',sheetname='Insurance')\n",
    "    income_sheet_GB = pd.read_excel(raw_data_path+'Income Statement.xls',sheetname='General Business')\n",
    "    print('Reading Cash Flow sheet use time:{}\\n'.format(time.time()-start_time))\n",
    "    income_sheet_GB\n",
    "\n",
    "\n",
    "    #-----------更名操作---------------------\n",
    "    '''\n",
    "    将三表中的一般工商、银行、保险、证券的科目具体标注，方便查看\n",
    "    sample as blew:\n",
    "    # balance_bank_old_columns=list(balance_sheet_Bank.columns[9:])\n",
    "    # balance_bank_new_columns=[x+'_balance_bank' for x in balance_bank_old_columns]\n",
    "    # balance_bank_columns_dic=dict(zip(balance_bank_old_columns,balance_bank_new_columns))\n",
    "    # balance_sheet_Bank.rename(columns=balance_bank_columns_dic,inplace=True)\n",
    "    '''\n",
    "    if whether_code==1:\n",
    "    \n",
    "        target_df=[balance_sheet_Bank,balance_sheet_Securities, balance_sheet_Insurance,balance_sheet_GB,\\\n",
    "                   cashFlow_sheet_Bank,cashFlow_sheet_Securities,cashFlow_sheet_Insurance,cashFlow_sheet_GB,\\\n",
    "                   income_sheet_Bank,income_sheet_Securities,income_sheet_Insurance,income_sheet_GB]\n",
    "        new_name=['_balance_Bank','_balance_Securities','_balance_Insurance','_balance_GB',\\\n",
    "                    '_cashFlow_Bank','_cashFlow_Securities','_cashFlow_Insurance','_cashFlow_GB',\\\n",
    "                  '_income_Bank','_income_Securities','_income_Insurance','_income_GB']\n",
    "        for (item,name) in zip(target_df,new_name):\n",
    "            old_columns=list(item.columns[9:])\n",
    "            new_columns=[x+name for x in old_columns]\n",
    "            columns_dic=dict(zip(old_columns,new_columns))\n",
    "            item.rename(columns=columns_dic,inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    return balance_sheet_Bank,balance_sheet_Securities, balance_sheet_Insurance,balance_sheet_GB,\\\n",
    "            cashFlow_sheet_Bank,cashFlow_sheet_Securities,cashFlow_sheet_Insurance,cashFlow_sheet_GB,\\\n",
    "           income_sheet_Bank,income_sheet_Securities,income_sheet_Insurance,income_sheet_GB\n",
    "\n",
    "##--------读取12表--------------------------\n",
    "def loading_12_file():\n",
    "    '''\n",
    "    读取12张表\n",
    "    '''\n",
    "    balance_sheet_Bank_old=pd.read_csv(file_path+'files/financial_data_new/balance_sheet_Bank.csv')\n",
    "    balance_sheet_Securities_old=pd.read_csv(file_path+'files/financial_data_new/balance_sheet_Securities.csv')\n",
    "    balance_sheet_Insurance_old=pd.read_csv(file_path+'files/financial_data_new/balance_sheet_Insurance.csv')\n",
    "    balance_sheet_GB_old=pd.read_csv(file_path+'files/financial_data_new/balance_sheet_GB.csv')\n",
    "\n",
    "    cashFlow_sheet_Bank_old=pd.read_csv(file_path+'files/financial_data_new/cashFlow_sheet_Bank.csv')\n",
    "    cashFlow_sheet_Securities_old=pd.read_csv(file_path+'files/financial_data_new/cashFlow_sheet_Securities.csv')\n",
    "    cashFlow_sheet_Insurance_old=pd.read_csv(file_path+'files/financial_data_new/cashFlow_sheet_Insurance.csv')\n",
    "    cashFlow_sheet_GB_old=pd.read_csv(file_path+'files/financial_data_new/cashFlow_sheet_GB.csv')\n",
    "\n",
    "    income_sheet_Bank_old=pd.read_csv(file_path+'files/financial_data_new/income_sheet_Bank.csv')\n",
    "    income_sheet_Securities_old=pd.read_csv(file_path+'files/financial_data_new/income_sheet_Securities.csv')\n",
    "    income_sheet_Insurance_old=pd.read_csv(file_path+'files/financial_data_new/income_sheet_Insurance.csv')\n",
    "    income_sheet_GB_old=pd.read_csv(file_path+'files/financial_data_new/income_sheet_GB.csv')\n",
    "    \n",
    "    return balance_sheet_Bank_old,balance_sheet_Securities_old,balance_sheet_Insurance_old,balance_sheet_GB_old,\\\n",
    "            cashFlow_sheet_Bank_old,cashFlow_sheet_Securities_old,cashFlow_sheet_Insurance_old,cashFlow_sheet_GB_old,\\\n",
    "            income_sheet_Bank_old,income_sheet_Securities_old,income_sheet_Insurance_old,income_sheet_GB_old\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def date_time(df):\n",
    "    df['END_DATE']=pd.to_datetime(df['END_DATE'],format='%Y/%m/%d')\n",
    "    df['PUBLISH_DATE']=pd.to_datetime(df['PUBLISH_DATE'],format='%Y/%m/%d')\n",
    "    df['END_DATE_REP']=pd.to_datetime(df['END_DATE_REP'],format='%Y/%m/%d')\n",
    "    df.sort_values(by=['TICKER_SYMBOL','END_DATE','PUBLISH_DATE','END_DATE_REP'],inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_3sheet():\n",
    "    \n",
    "    '''\n",
    "    将12表，四四合成，形成三表，并做point_in_time处理\n",
    "    \n",
    "    return  合好的三表【仅有'TICKER_SYMBOL','PUBLISH_DATE','END_DATE_REP','END_DATE','MERGED_FLAG'这几列】\n",
    "    '''\n",
    "\n",
    "    balance_sheet_Bank,balance_sheet_Securities, balance_sheet_Insurance,balance_sheet_GB,\\\n",
    "    cashFlow_sheet_Bank,cashFlow_sheet_Securities,cashFlow_sheet_Insurance,cashFlow_sheet_GB,\\\n",
    "    income_sheet_Bank,income_sheet_Securities,income_sheet_Insurance,income_sheet_GB=loading_12_file()\n",
    "\n",
    "    common_list=['TICKER_SYMBOL','PUBLISH_DATE','END_DATE_REP','END_DATE','MERGED_FLAG']\n",
    "\n",
    "    merge_list_balance=[balance_sheet_Bank[common_list],balance_sheet_Securities[common_list], balance_sheet_Insurance[common_list],balance_sheet_GB[common_list]]\n",
    "    merge_list_cashFlow=[cashFlow_sheet_Bank[common_list],cashFlow_sheet_Securities[common_list],cashFlow_sheet_Insurance[common_list],cashFlow_sheet_GB[common_list]]\n",
    "    merge_list_income=[income_sheet_Bank[common_list],income_sheet_Securities[common_list],income_sheet_Insurance[common_list],income_sheet_GB[common_list]]\n",
    "\n",
    "\n",
    "    Merge_Balance=pd.concat(merge_list_balance)\n",
    "    Merge_CashFlow=pd.concat(merge_list_cashFlow)\n",
    "    Merge_Income=pd.concat(merge_list_income)\n",
    "\n",
    "    Merge_Balance=date_time(Merge_Balance)\n",
    "    Merge_CashFlow=date_time(Merge_CashFlow)\n",
    "    Merge_Income=date_time(Merge_Income)\n",
    "\n",
    "    Merge_Balance.drop_duplicates(subset=['TICKER_SYMBOL','END_DATE'], keep='last',inplace=True)\n",
    "    Merge_CashFlow.drop_duplicates(subset=['TICKER_SYMBOL','END_DATE'], keep='last',inplace=True)\n",
    "    Merge_Income.drop_duplicates(subset=['TICKER_SYMBOL','END_DATE'], keep='last',inplace=True)\n",
    "    \n",
    "    return Merge_Balance,Merge_CashFlow,Merge_Income\n",
    "\n",
    "\n",
    "\n",
    "    ##check ticker的三表情况\n",
    "    \n",
    "#生成基准上市之后的ticker-时间 df\n",
    "\n",
    "def ticker_3sheet_info(Merge_Balance,Merge_CashFlow,Merge_Income):\n",
    "    '''\n",
    "    输入是合好并且去重的三表，生成上市公司三表统计，注意有30个ticker缺乏上市时间，未在列\n",
    "    \n",
    "    \n",
    "    [  67, 1043, 1226, 1236, 1426, 1796, 1888, 2074, 2151, 2164, 2165,\n",
    "            2166, 2167, 2168, 3001, 3013, 3028, 3122, 3150, 3238, 3252, 3253,\n",
    "            3280, 3314, 3351, 3355, 3426, 3434, 3478, 3494]\n",
    "    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    list_date=pd.read_csv(file_path+'ticker_market.csv',encoding='gbk')\n",
    "    list_date.dropna(subset=['listDate'],inplace=True)\n",
    "    #list_date['TICKER_SYMBOL']=list_date['TICKER_SYMBOL'].apply(lambda x:\"%06d\"%x)\n",
    "    list_date_dic=dict(zip(list_date.TICKER_SYMBOL,list_date.listDate))\n",
    "    list_date_dic\n",
    "\n",
    "    range_list=['2009-03-31', '2009-06-30', '2009-09-30', '2009-12-31',\\\n",
    "        '2010-03-31', '2010-06-30', '2010-09-30', '2010-12-31',\\\n",
    "        '2011-03-31', '2011-06-30', '2011-09-30', '2011-12-31',\\\n",
    "        '2012-03-31', '2012-06-30', '2012-09-30', '2012-12-31',\\\n",
    "        '2013-03-31', '2013-06-30', '2013-09-30', '2013-12-31',\\\n",
    "        '2014-03-31', '2014-06-30', '2014-09-30', '2014-12-31',\\\n",
    "        '2015-03-31', '2015-06-30', '2015-09-30', '2015-12-31',\\\n",
    "        '2016-03-31', '2016-06-30', '2016-09-30', '2016-12-31',\\\n",
    "        '2017-03-31', '2017-06-30', '2017-09-30', '2017-12-31',\\\n",
    "        '2018-03-31']\n",
    "\n",
    "\n",
    "    for ticker in list_date_dic.keys():\n",
    "        for i in range(len(range_list)-1):\n",
    "            if list_date_dic[ticker]<=range_list[0]:\n",
    "                list_date_dic[ticker]=range_list[0]\n",
    "                break\n",
    "            elif (list_date_dic[ticker]>range_list[i])&(list_date_dic[ticker]<=range_list[i+1]):\n",
    "                list_date_dic[ticker]=range_list[i+1]\n",
    "    list_date_dic\n",
    "\n",
    "\n",
    "    date_dic=dict()\n",
    "    for i in range(len(range_list)):\n",
    "        date_dic[range_list[i]]=range_list[i:]\n",
    "    date_dic\n",
    "    list_date_dic\n",
    "\n",
    "\n",
    "    DATE=[]\n",
    "    TICKER=[]\n",
    "    for ticker in list_date_dic.keys():\n",
    "        TICKER.extend([ticker for x in range(len(date_dic[list_date_dic[ticker]]))])\n",
    "        DATE.extend(date_dic[list_date_dic[ticker]])\n",
    "\n",
    "    BASE_list=DataFrame({'TICKER_SYMBOL':TICKER,'END_DATE':DATE})\n",
    "    BASE_list['END_DATE']=pd.to_datetime(BASE_list['END_DATE'],format='%Y/%m/%d')\n",
    "    BASE_list\n",
    "\n",
    "\n",
    "    # 将三表分别merge基准df中\n",
    "\n",
    "    Merge_Balance.rename(columns={'MERGED_FLAG':'Balance'},inplace=True)\n",
    "    Merge_CashFlow.rename(columns={'MERGED_FLAG':'CashFlow'},inplace=True)\n",
    "    Merge_Income.rename(columns={'MERGED_FLAG':'Income'},inplace=True)\n",
    "\n",
    "    BASE1=pd.merge(BASE_list,Merge_Balance[['TICKER_SYMBOL','END_DATE','Balance']],how='left',on=['TICKER_SYMBOL','END_DATE'])\n",
    "    BASE2=pd.merge(BASE1,Merge_CashFlow[['TICKER_SYMBOL','END_DATE','CashFlow']],how='left',on=['TICKER_SYMBOL','END_DATE'])\n",
    "    BASE3=pd.merge(BASE2,Merge_Income[['TICKER_SYMBOL','END_DATE','Income']],how='left',on=['TICKER_SYMBOL','END_DATE'])\n",
    "    BASE3.to_csv(file_path+'上市公司_三表统计.csv',index=False)\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def check_uniqueness():\n",
    "    \n",
    "    '''\n",
    "   生成三日期联立，不唯一的数据记录\n",
    "   如要检查不唯一数据是否值相等，则可用'3_DATE_唯一性检查.csv，提取出在不同表中的数据，用原表merge，再去重，若去重前后数据多了，\n",
    "   则找出重复的\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #读取12表\n",
    "    balance_sheet_Bank,balance_sheet_Securities, balance_sheet_Insurance,balance_sheet_GB,\\\n",
    "    cashFlow_sheet_Bank,cashFlow_sheet_Securities,cashFlow_sheet_Insurance,cashFlow_sheet_GB,\\\n",
    "    income_sheet_Bank,income_sheet_Securities,income_sheet_Insurance,income_sheet_GB=loading_12_file()\n",
    "    \n",
    "    \n",
    "\n",
    "    sheet_list=['Balance','Balance','Balance','Balance','cashFlow','cashFlow','cashFlow','cashFlow','Income','Income','Income','Income']\n",
    "\n",
    "    cate_list=['Bank','Securities','Insurance','GB','Bank','Securities','Insurance','GB','Bank','Securities','Insurance','GB']\n",
    "\n",
    "    T=[]\n",
    "\n",
    "    for df,sheet_name,cate_name in zip(df_list,sheet_list,cate_list):\n",
    "        t=df.groupby(['TICKER_SYMBOL','PUBLISH_DATE','END_DATE_REP','END_DATE']).size()\n",
    "        t_new=t[t!=1].reset_index()\n",
    "        t_new['SHEET']=sheet_name\n",
    "        t_new['CATEGORY']=cate_name\n",
    "        T.append(t_new)\n",
    "\n",
    "    pd.concat(T).to_csv(file_path+'3_DATE_唯一性检查.csv',index=False)\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def date_time_index(df):\n",
    "    '''\n",
    "    datatime函数排序的时候加上了index\n",
    "    '''\n",
    "    df['END_DATE']=pd.to_datetime(df['END_DATE'],format='%Y/%m/%d')\n",
    "    df['PUBLISH_DATE']=pd.to_datetime(df['PUBLISH_DATE'],format='%Y/%m/%d')\n",
    "    df['END_DATE_REP']=pd.to_datetime(df['END_DATE_REP'],format='%Y/%m/%d')\n",
    "    df.sort_values(by=['TICKER_SYMBOL','END_DATE','PUBLISH_DATE','END_DATE_REP','INDEX'],inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_nan_PIT():\n",
    "    \n",
    "    '''\n",
    "    将12表的point in time 最新数据缺失值补全\n",
    "    '''\n",
    "\n",
    "    balance_sheet_Bank_old,balance_sheet_Securities_old,balance_sheet_Insurance_old,balance_sheet_GB_old,\\\n",
    "    cashFlow_sheet_Bank_old,cashFlow_sheet_Securities_old,cashFlow_sheet_Insurance_old,cashFlow_sheet_GB_old,\\\n",
    "    income_sheet_Bank_old,income_sheet_Securities_old,income_sheet_Insurance_old,income_sheet_GB_old=loading_12_file()\n",
    "\n",
    "    balance_sheet_Bank_old['INDEX']=[x for x in range(len(balance_sheet_Bank_old))]\n",
    "    balance_sheet_Bank_old\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    files= os.listdir(split_file_path) \n",
    "\n",
    "    for file in files:\n",
    "        feature = pd.read_csv(split_file_path+file)\n",
    "        print(file)\n",
    "        feature['INDEX']=[x for x in range(len(feature))]\n",
    "        T=date_time_index(feature)\n",
    "        T.columns\n",
    "\n",
    "        common_col=['PARTY_ID', 'TICKER_SYMBOL', 'EXCHANGE_CD', 'PUBLISH_DATE',\n",
    "                 'END_DATE_REP', 'END_DATE', 'REPORT_TYPE', 'FISCAL_PERIOD',\n",
    "                 'MERGED_FLAG','INDEX']\n",
    "        rest_col=set(T.columns)-set(common_col)\n",
    "        rest_col\n",
    "\n",
    "        mid_1=T[common_col].drop_duplicates(subset=['TICKER_SYMBOL','END_DATE'])\n",
    "        mid_1['END_DATE']=mid_1['END_DATE'].apply(lambda x:datetime.datetime.strftime(x,'%Y-%m-%d'))\n",
    "        ticker_date=zip(mid_1.TICKER_SYMBOL,mid_1.END_DATE)\n",
    "        ticker_date\n",
    "\n",
    "\n",
    "        start_time=time.time()\n",
    "\n",
    "        Ticker=[]\n",
    "        Enddate=[]\n",
    "        Col=[]\n",
    "        Value=[]\n",
    "\n",
    "        for (a,b) in ticker_date:\n",
    "\n",
    "            #print(a,b)\n",
    "            mid_2=T[(T.TICKER_SYMBOL==a)&(T.END_DATE==b)]   \n",
    "            mid_2\n",
    "            for col in rest_col:\n",
    "                #print(col)\n",
    "                #common_col.extend(rest_col)\n",
    "                #mid_2[common_col]\n",
    "\n",
    "                ##当长度为1，直接跳出；当长度不是1，但全是nan或者全0，全部跳出;然后常规判断，当最新消息为空或零，依次往上找，直到找到最近的有值的\n",
    "                if (len(mid_2[col].values)==1):\n",
    "                    continue\n",
    "                elif ((np.isnan(mid_2[col].unique()[0])or(mid_2[col].unique()[0]==0))&(len(mid_2[col].unique())==1)):\n",
    "                    continue  \n",
    "                else:\n",
    "                    i=len(mid_2[col])-1\n",
    "\n",
    "                    while((np.isnan(mid_2[col].values[i])or(mid_2[col].values[i]==0))&(i>0)):\n",
    "                        i-=1\n",
    "\n",
    "                    if i==(len(mid_2[col])-1):\n",
    "                        continue\n",
    "                    else:\n",
    "                        #print(1)\n",
    "                        Index=T[(T.TICKER_SYMBOL==a)&(T.END_DATE==b)].index[-1]\n",
    "                        T.loc[[Index],col]=mid_2[col].values[i]\n",
    "                        Ticker.append(a)\n",
    "                        Enddate.append(b)\n",
    "                        Col.append(col)\n",
    "                        Value.append(mid_2[col].values[i])\n",
    "\n",
    "                    #T[(T.TICKER_SYMBOL==1)&(T.END_DATE=='2008-12-31')].loc[Index,'AA']\n",
    "        DataFrame({'ticker': Ticker,'enddate':Enddate,'col':Col,'value': Value}).to_csv(file_path+'fix_bug/record_%s.csv'%file[0:-4],index=False)\n",
    "        T.to_csv(file_path+'fix_bug/%s_new.csv'%file[0:-4],index=False)\n",
    "        print('dealing 1st sheet use time:{}\\n'.format(time.time()-start_time))\n",
    "        del T\n",
    "        gc.collect()\n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
