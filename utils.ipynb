{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import time\n",
    "import math\n",
    "import datetime\n",
    "import os,gc\n",
    "file_path='D:/360WiFi/'\n",
    "raw_data_path='D:/360WiFi/FDDC_financial_data_20180613/'\n",
    "split_file_path='D:/360WiFi/files/financial_data_new/'\n",
    "input_path='D:/360WiFi/files/input/'\n",
    "mid_path='D:/360WiFi/files/mid_file/'\n",
    "other_path='D:/360WiFi/files/other_file/'\n",
    "'''\n",
    "Function List:\n",
    "\n",
    "    loadingFile:读取原始文件\n",
    "    loading_12_file:读取12表\n",
    "    date_time：将表中三日期datatime化,并排序\n",
    "    date_time_index：date_time排序的时候加上INDEX\n",
    "    merge_3sheet：合成三大表\n",
    "    ticker_3sheet_info：检查上市后ticker是否有三表缺失情况\n",
    "    check_uniqueness：检查三DATE联立后是否有冲突数据\n",
    "    fill_nan_PIT:将12表的point in time 最新数据缺失值补全\n",
    "    \n",
    "    preprocess:合三表（涉及到）preprocess_common，preprocess_normal，preprocess_special，\n",
    "                              其中preprocess_common又涉及到loading_inputs\n",
    "                              \n",
    "                              \n",
    "    check_revenue():生成REVENUE_commom_income和当季revenue的对照check_revence.csv\n",
    "    get_weight():得到权重列\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "def loadingFile(whether_code):\n",
    "    '''\n",
    "    whether_code=1,替换columns\n",
    "    whether_code=0，不替换columns\n",
    "    \n",
    "    \n",
    "    return 12 original files \n",
    "    use time: 25mins\n",
    "    '''\n",
    "    #------处理 BalanceSheet---------------------\n",
    "    \n",
    "    start_time=time.time()\n",
    "    balance_sheet_Bank = pd.read_excel(raw_data_path+'Balance Sheet.xls',sheetname='Bank')\n",
    "    balance_sheet_Securities = pd.read_excel(raw_data_path+'Balance Sheet.xls',sheetname='Securities')\n",
    "    balance_sheet_Insurance = pd.read_excel(raw_data_path+'Balance Sheet.xls',sheetname='Insurance')\n",
    "    balance_sheet_GB = pd.read_excel(raw_data_path+'Balance Sheet.xls',sheetname='General Business')\n",
    "\n",
    "    print('Reading Balance sheet use time:{}\\n'.format(time.time()-start_time))\n",
    "\n",
    "\n",
    "    #处理---------- Cash Flow Statement--------------------\n",
    "    \n",
    "    start_time=time.time()\n",
    "    cashFlow_sheet_Bank = pd.read_excel(raw_data_path+'Cash Flow Statement.xls',sheetname='Bank')\n",
    "    cashFlow_sheet_Securities = pd.read_excel(raw_data_path+'Cash Flow Statement.xls',sheetname='Securities')\n",
    "    cashFlow_sheet_Insurance = pd.read_excel(raw_data_path+'Cash Flow Statement.xls',sheetname='Insurance')\n",
    "    cashFlow_sheet_GB = pd.read_excel(raw_data_path+'Cash Flow Statement.xls',sheetname='General Business')\n",
    "    print('Reading Cash Flow sheet use time:{}\\n'.format(time.time()-start_time))\n",
    "    cashFlow_sheet_GB\n",
    "\n",
    "\n",
    "    #处理 ----------Income Statement------------------------\n",
    "\n",
    "    start_time=time.time()\n",
    "    income_sheet_Bank = pd.read_excel(raw_data_path+'Income Statement.xls',sheetname='Bank')\n",
    "    income_sheet_Securities = pd.read_excel(raw_data_path+'Income Statement.xls',sheetname='Securities')\n",
    "    income_sheet_Insurance = pd.read_excel(raw_data_path+'Income Statement.xls',sheetname='Insurance')\n",
    "    income_sheet_GB = pd.read_excel(raw_data_path+'Income Statement.xls',sheetname='General Business')\n",
    "    print('Reading Cash Flow sheet use time:{}\\n'.format(time.time()-start_time))\n",
    "    income_sheet_GB\n",
    "\n",
    "\n",
    "    #-----------更名操作---------------------\n",
    "    '''\n",
    "    将三表中的一般工商、银行、保险、证券的科目具体标注，方便查看\n",
    "    sample as blew:\n",
    "    # balance_bank_old_columns=list(balance_sheet_Bank.columns[9:])\n",
    "    # balance_bank_new_columns=[x+'_balance_bank' for x in balance_bank_old_columns]\n",
    "    # balance_bank_columns_dic=dict(zip(balance_bank_old_columns,balance_bank_new_columns))\n",
    "    # balance_sheet_Bank.rename(columns=balance_bank_columns_dic,inplace=True)\n",
    "    '''\n",
    "    if whether_code==1:\n",
    "    \n",
    "        target_df=[balance_sheet_Bank,balance_sheet_Securities, balance_sheet_Insurance,balance_sheet_GB,\\\n",
    "                   cashFlow_sheet_Bank,cashFlow_sheet_Securities,cashFlow_sheet_Insurance,cashFlow_sheet_GB,\\\n",
    "                   income_sheet_Bank,income_sheet_Securities,income_sheet_Insurance,income_sheet_GB]\n",
    "        new_name=['_balance_Bank','_balance_Securities','_balance_Insurance','_balance_GB',\\\n",
    "                    '_cashFlow_Bank','_cashFlow_Securities','_cashFlow_Insurance','_cashFlow_GB',\\\n",
    "                  '_income_Bank','_income_Securities','_income_Insurance','_income_GB']\n",
    "        for (item,name) in zip(target_df,new_name):\n",
    "            old_columns=list(item.columns[9:])\n",
    "            new_columns=[x+name for x in old_columns]\n",
    "            columns_dic=dict(zip(old_columns,new_columns))\n",
    "            item.rename(columns=columns_dic,inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    return balance_sheet_Bank,balance_sheet_Securities, balance_sheet_Insurance,balance_sheet_GB,\\\n",
    "            cashFlow_sheet_Bank,cashFlow_sheet_Securities,cashFlow_sheet_Insurance,cashFlow_sheet_GB,\\\n",
    "           income_sheet_Bank,income_sheet_Securities,income_sheet_Insurance,income_sheet_GB\n",
    "\n",
    "##--------读取12表--------------------------\n",
    "def loading_12_file():\n",
    "    '''\n",
    "    读取12张表\n",
    "    '''\n",
    "    balance_sheet_Bank_old=pd.read_csv(file_path+'files/financial_data_new/balance_sheet_Bank.csv')\n",
    "    balance_sheet_Securities_old=pd.read_csv(file_path+'files/financial_data_new/balance_sheet_Securities.csv')\n",
    "    balance_sheet_Insurance_old=pd.read_csv(file_path+'files/financial_data_new/balance_sheet_Insurance.csv')\n",
    "    balance_sheet_GB_old=pd.read_csv(file_path+'files/financial_data_new/balance_sheet_GB.csv')\n",
    "\n",
    "    cashFlow_sheet_Bank_old=pd.read_csv(file_path+'files/financial_data_new/cashFlow_sheet_Bank.csv')\n",
    "    cashFlow_sheet_Securities_old=pd.read_csv(file_path+'files/financial_data_new/cashFlow_sheet_Securities.csv')\n",
    "    cashFlow_sheet_Insurance_old=pd.read_csv(file_path+'files/financial_data_new/cashFlow_sheet_Insurance.csv')\n",
    "    cashFlow_sheet_GB_old=pd.read_csv(file_path+'files/financial_data_new/cashFlow_sheet_GB.csv')\n",
    "\n",
    "    income_sheet_Bank_old=pd.read_csv(file_path+'files/financial_data_new/income_sheet_Bank.csv')\n",
    "    income_sheet_Securities_old=pd.read_csv(file_path+'files/financial_data_new/income_sheet_Securities.csv')\n",
    "    income_sheet_Insurance_old=pd.read_csv(file_path+'files/financial_data_new/income_sheet_Insurance.csv')\n",
    "    income_sheet_GB_old=pd.read_csv(file_path+'files/financial_data_new/income_sheet_GB.csv')\n",
    "    \n",
    "    return balance_sheet_Bank_old,balance_sheet_Securities_old,balance_sheet_Insurance_old,balance_sheet_GB_old,\\\n",
    "            cashFlow_sheet_Bank_old,cashFlow_sheet_Securities_old,cashFlow_sheet_Insurance_old,cashFlow_sheet_GB_old,\\\n",
    "            income_sheet_Bank_old,income_sheet_Securities_old,income_sheet_Insurance_old,income_sheet_GB_old\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def date_time(df):\n",
    "    df['END_DATE']=pd.to_datetime(df['END_DATE'],format='%Y/%m/%d')\n",
    "    df['PUBLISH_DATE']=pd.to_datetime(df['PUBLISH_DATE'],format='%Y/%m/%d')\n",
    "    df['END_DATE_REP']=pd.to_datetime(df['END_DATE_REP'],format='%Y/%m/%d')\n",
    "    df.sort_values(by=['TICKER_SYMBOL','END_DATE','PUBLISH_DATE','END_DATE_REP'],inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_3sheet():\n",
    "    \n",
    "    '''\n",
    "    将12表，四四合成，形成三表，并做point_in_time处理\n",
    "    \n",
    "    return  合好的三表【仅有'TICKER_SYMBOL','PUBLISH_DATE','END_DATE_REP','END_DATE','MERGED_FLAG'这几列】\n",
    "    '''\n",
    "\n",
    "    balance_sheet_Bank,balance_sheet_Securities, balance_sheet_Insurance,balance_sheet_GB,\\\n",
    "    cashFlow_sheet_Bank,cashFlow_sheet_Securities,cashFlow_sheet_Insurance,cashFlow_sheet_GB,\\\n",
    "    income_sheet_Bank,income_sheet_Securities,income_sheet_Insurance,income_sheet_GB=loading_12_file()\n",
    "\n",
    "    common_list=['TICKER_SYMBOL','PUBLISH_DATE','END_DATE_REP','END_DATE','MERGED_FLAG']\n",
    "\n",
    "    merge_list_balance=[balance_sheet_Bank[common_list],balance_sheet_Securities[common_list], balance_sheet_Insurance[common_list],balance_sheet_GB[common_list]]\n",
    "    merge_list_cashFlow=[cashFlow_sheet_Bank[common_list],cashFlow_sheet_Securities[common_list],cashFlow_sheet_Insurance[common_list],cashFlow_sheet_GB[common_list]]\n",
    "    merge_list_income=[income_sheet_Bank[common_list],income_sheet_Securities[common_list],income_sheet_Insurance[common_list],income_sheet_GB[common_list]]\n",
    "\n",
    "\n",
    "    Merge_Balance=pd.concat(merge_list_balance)\n",
    "    Merge_CashFlow=pd.concat(merge_list_cashFlow)\n",
    "    Merge_Income=pd.concat(merge_list_income)\n",
    "\n",
    "    Merge_Balance=date_time(Merge_Balance)\n",
    "    Merge_CashFlow=date_time(Merge_CashFlow)\n",
    "    Merge_Income=date_time(Merge_Income)\n",
    "\n",
    "    Merge_Balance.drop_duplicates(subset=['TICKER_SYMBOL','END_DATE'], keep='last',inplace=True)\n",
    "    Merge_CashFlow.drop_duplicates(subset=['TICKER_SYMBOL','END_DATE'], keep='last',inplace=True)\n",
    "    Merge_Income.drop_duplicates(subset=['TICKER_SYMBOL','END_DATE'], keep='last',inplace=True)\n",
    "    \n",
    "    return Merge_Balance,Merge_CashFlow,Merge_Income\n",
    "\n",
    "\n",
    "\n",
    "    ##check ticker的三表情况\n",
    "    \n",
    "#生成基准上市之后的ticker-时间 df\n",
    "\n",
    "def ticker_3sheet_info(Merge_Balance,Merge_CashFlow,Merge_Income):\n",
    "    '''\n",
    "    输入是合好并且去重的三表，生成上市公司三表统计，注意有30个ticker缺乏上市时间，未在列\n",
    "    \n",
    "    \n",
    "    [  67, 1043, 1226, 1236, 1426, 1796, 1888, 2074, 2151, 2164, 2165,\n",
    "            2166, 2167, 2168, 3001, 3013, 3028, 3122, 3150, 3238, 3252, 3253,\n",
    "            3280, 3314, 3351, 3355, 3426, 3434, 3478, 3494]\n",
    "    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    list_date=pd.read_csv(file_path+'ticker_market.csv',encoding='gbk')\n",
    "    list_date.dropna(subset=['listDate'],inplace=True)\n",
    "    #list_date['TICKER_SYMBOL']=list_date['TICKER_SYMBOL'].apply(lambda x:\"%06d\"%x)\n",
    "    list_date_dic=dict(zip(list_date.TICKER_SYMBOL,list_date.listDate))\n",
    "    list_date_dic\n",
    "\n",
    "    range_list=['2009-03-31', '2009-06-30', '2009-09-30', '2009-12-31',\\\n",
    "        '2010-03-31', '2010-06-30', '2010-09-30', '2010-12-31',\\\n",
    "        '2011-03-31', '2011-06-30', '2011-09-30', '2011-12-31',\\\n",
    "        '2012-03-31', '2012-06-30', '2012-09-30', '2012-12-31',\\\n",
    "        '2013-03-31', '2013-06-30', '2013-09-30', '2013-12-31',\\\n",
    "        '2014-03-31', '2014-06-30', '2014-09-30', '2014-12-31',\\\n",
    "        '2015-03-31', '2015-06-30', '2015-09-30', '2015-12-31',\\\n",
    "        '2016-03-31', '2016-06-30', '2016-09-30', '2016-12-31',\\\n",
    "        '2017-03-31', '2017-06-30', '2017-09-30', '2017-12-31',\\\n",
    "        '2018-03-31']\n",
    "\n",
    "\n",
    "    for ticker in list_date_dic.keys():\n",
    "        for i in range(len(range_list)-1):\n",
    "            if list_date_dic[ticker]<=range_list[0]:\n",
    "                list_date_dic[ticker]=range_list[0]\n",
    "                break\n",
    "            elif (list_date_dic[ticker]>range_list[i])&(list_date_dic[ticker]<=range_list[i+1]):\n",
    "                list_date_dic[ticker]=range_list[i+1]\n",
    "    list_date_dic\n",
    "\n",
    "\n",
    "    date_dic=dict()\n",
    "    for i in range(len(range_list)):\n",
    "        date_dic[range_list[i]]=range_list[i:]\n",
    "    date_dic\n",
    "    list_date_dic\n",
    "\n",
    "\n",
    "    DATE=[]\n",
    "    TICKER=[]\n",
    "    for ticker in list_date_dic.keys():\n",
    "        TICKER.extend([ticker for x in range(len(date_dic[list_date_dic[ticker]]))])\n",
    "        DATE.extend(date_dic[list_date_dic[ticker]])\n",
    "\n",
    "    BASE_list=DataFrame({'TICKER_SYMBOL':TICKER,'END_DATE':DATE})\n",
    "    BASE_list['END_DATE']=pd.to_datetime(BASE_list['END_DATE'],format='%Y/%m/%d')\n",
    "    BASE_list\n",
    "\n",
    "\n",
    "    # 将三表分别merge基准df中\n",
    "\n",
    "    Merge_Balance.rename(columns={'MERGED_FLAG':'Balance'},inplace=True)\n",
    "    Merge_CashFlow.rename(columns={'MERGED_FLAG':'CashFlow'},inplace=True)\n",
    "    Merge_Income.rename(columns={'MERGED_FLAG':'Income'},inplace=True)\n",
    "\n",
    "    BASE1=pd.merge(BASE_list,Merge_Balance[['TICKER_SYMBOL','END_DATE','Balance']],how='left',on=['TICKER_SYMBOL','END_DATE'])\n",
    "    BASE2=pd.merge(BASE1,Merge_CashFlow[['TICKER_SYMBOL','END_DATE','CashFlow']],how='left',on=['TICKER_SYMBOL','END_DATE'])\n",
    "    BASE3=pd.merge(BASE2,Merge_Income[['TICKER_SYMBOL','END_DATE','Income']],how='left',on=['TICKER_SYMBOL','END_DATE'])\n",
    "    BASE3.to_csv(file_path+'上市公司_三表统计.csv',index=False)\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def check_uniqueness():\n",
    "    \n",
    "    '''\n",
    "   生成三日期联立，不唯一的数据记录\n",
    "   如要检查不唯一数据是否值相等，则可用'3_DATE_唯一性检查.csv，提取出在不同表中的数据，用原表merge，再去重，若去重前后数据多了，\n",
    "   则找出重复的\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #读取12表\n",
    "    balance_sheet_Bank,balance_sheet_Securities, balance_sheet_Insurance,balance_sheet_GB,\\\n",
    "    cashFlow_sheet_Bank,cashFlow_sheet_Securities,cashFlow_sheet_Insurance,cashFlow_sheet_GB,\\\n",
    "    income_sheet_Bank,income_sheet_Securities,income_sheet_Insurance,income_sheet_GB=loading_12_file()\n",
    "    \n",
    "    \n",
    "\n",
    "    sheet_list=['Balance','Balance','Balance','Balance','cashFlow','cashFlow','cashFlow','cashFlow','Income','Income','Income','Income']\n",
    "\n",
    "    cate_list=['Bank','Securities','Insurance','GB','Bank','Securities','Insurance','GB','Bank','Securities','Insurance','GB']\n",
    "\n",
    "    T=[]\n",
    "\n",
    "    for df,sheet_name,cate_name in zip(df_list,sheet_list,cate_list):\n",
    "        t=df.groupby(['TICKER_SYMBOL','PUBLISH_DATE','END_DATE_REP','END_DATE']).size()\n",
    "        t_new=t[t!=1].reset_index()\n",
    "        t_new['SHEET']=sheet_name\n",
    "        t_new['CATEGORY']=cate_name\n",
    "        T.append(t_new)\n",
    "\n",
    "    pd.concat(T).to_csv(file_path+'3_DATE_唯一性检查.csv',index=False)\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def date_time_index(df):\n",
    "    '''\n",
    "    datatime函数排序的时候加上了index\n",
    "    '''\n",
    "    df['END_DATE']=pd.to_datetime(df['END_DATE'],format='%Y/%m/%d')\n",
    "    df['PUBLISH_DATE']=pd.to_datetime(df['PUBLISH_DATE'],format='%Y/%m/%d')\n",
    "    df['END_DATE_REP']=pd.to_datetime(df['END_DATE_REP'],format='%Y/%m/%d')\n",
    "    df.sort_values(by=['TICKER_SYMBOL','END_DATE','PUBLISH_DATE','END_DATE_REP','INDEX'],inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_nan_PIT():\n",
    "    \n",
    "    '''\n",
    "    将12表的point in time 最新数据缺失值补全\n",
    "    '''\n",
    "\n",
    "    balance_sheet_Bank_old,balance_sheet_Securities_old,balance_sheet_Insurance_old,balance_sheet_GB_old,\\\n",
    "    cashFlow_sheet_Bank_old,cashFlow_sheet_Securities_old,cashFlow_sheet_Insurance_old,cashFlow_sheet_GB_old,\\\n",
    "    income_sheet_Bank_old,income_sheet_Securities_old,income_sheet_Insurance_old,income_sheet_GB_old=loading_12_file()\n",
    "\n",
    "    balance_sheet_Bank_old['INDEX']=[x for x in range(len(balance_sheet_Bank_old))]\n",
    "    balance_sheet_Bank_old\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    files= os.listdir(split_file_path) \n",
    "\n",
    "    for file in files:\n",
    "        feature = pd.read_csv(split_file_path+file)\n",
    "        print(file)\n",
    "        feature['INDEX']=[x for x in range(len(feature))]\n",
    "        T=date_time_index(feature)\n",
    "        T.columns\n",
    "\n",
    "        common_col=['PARTY_ID', 'TICKER_SYMBOL', 'EXCHANGE_CD', 'PUBLISH_DATE',\n",
    "                 'END_DATE_REP', 'END_DATE', 'REPORT_TYPE', 'FISCAL_PERIOD',\n",
    "                 'MERGED_FLAG','INDEX']\n",
    "        rest_col=set(T.columns)-set(common_col)\n",
    "        rest_col\n",
    "\n",
    "        mid_1=T[common_col].drop_duplicates(subset=['TICKER_SYMBOL','END_DATE'])\n",
    "        mid_1['END_DATE']=mid_1['END_DATE'].apply(lambda x:datetime.datetime.strftime(x,'%Y-%m-%d'))\n",
    "        ticker_date=zip(mid_1.TICKER_SYMBOL,mid_1.END_DATE)\n",
    "        ticker_date\n",
    "\n",
    "\n",
    "        start_time=time.time()\n",
    "\n",
    "        Ticker=[]\n",
    "        Enddate=[]\n",
    "        Col=[]\n",
    "        Value=[]\n",
    "\n",
    "        for (a,b) in ticker_date:\n",
    "\n",
    "            #print(a,b)\n",
    "            mid_2=T[(T.TICKER_SYMBOL==a)&(T.END_DATE==b)]   \n",
    "            mid_2\n",
    "            for col in rest_col:\n",
    "                #print(col)\n",
    "                #common_col.extend(rest_col)\n",
    "                #mid_2[common_col]\n",
    "\n",
    "                ##当长度为1，直接跳出；当长度不是1，但全是nan或者全0，全部跳出;然后常规判断，当最新消息为空或零，依次往上找，直到找到最近的有值的\n",
    "                if (len(mid_2[col].values)==1):\n",
    "                    continue\n",
    "                elif ((np.isnan(mid_2[col].unique()[0])or(mid_2[col].unique()[0]==0))&(len(mid_2[col].unique())==1)):\n",
    "                    continue  \n",
    "                else:\n",
    "                    i=len(mid_2[col])-1\n",
    "\n",
    "                    while((np.isnan(mid_2[col].values[i])or(mid_2[col].values[i]==0))&(i>0)):\n",
    "                        i-=1\n",
    "\n",
    "                    if i==(len(mid_2[col])-1):\n",
    "                        continue\n",
    "                    else:\n",
    "                        #print(1)\n",
    "                        Index=T[(T.TICKER_SYMBOL==a)&(T.END_DATE==b)].index[-1]\n",
    "                        T.loc[[Index],col]=mid_2[col].values[i]\n",
    "                        Ticker.append(a)\n",
    "                        Enddate.append(b)\n",
    "                        Col.append(col)\n",
    "                        Value.append(mid_2[col].values[i])\n",
    "\n",
    "                    #T[(T.TICKER_SYMBOL==1)&(T.END_DATE=='2008-12-31')].loc[Index,'AA']\n",
    "        DataFrame({'ticker': Ticker,'enddate':Enddate,'col':Col,'value': Value}).to_csv(file_path+'fix_bug/record_%s.csv'%file[0:-4],index=False)\n",
    "        T.to_csv(file_path+'fix_bug/%s_new.csv'%file[0:-4],index=False)\n",
    "        print('dealing 1st sheet use time:{}\\n'.format(time.time()-start_time))\n",
    "        del T\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "def check_revenue():\n",
    "    '''\n",
    "    生成'REVENUE_commom_income'和当季revenue的对照check_revence.csv\n",
    "    '''\n",
    "    table=pd.read_csv(mid_path+'table_6_file_label.csv')\n",
    "    ##table=preprocess.drop_rows(table)\n",
    "    table=table[['TICKER_SYMBOL','year','month','REVENUE_commom_income','label']]\n",
    "    table.to_csv(other_path+'check_revence.csv',index=False)\n",
    "    print(table.shape)\n",
    "    return \n",
    "    \n",
    "    \n",
    "def ticker_revenue():\n",
    "    '''\n",
    "    生成ticker_revenue对应表\n",
    "    \n",
    "    '''\n",
    "    M2=pd.read_csv(mid_path+'table_base.csv')\n",
    "    submit=pd.read_csv(file_path+'sumbit_v1.csv')\n",
    "    base=submit[['TICKER_SYMBOL']]\n",
    "    base\n",
    "\n",
    "    time_range=['2009-03-31', '2009-06-30', '2009-09-30', '2009-12-31',\n",
    "           '2010-03-31', '2010-06-30', '2010-09-30', '2010-12-31',\n",
    "           '2011-03-31', '2011-06-30', '2011-09-30', '2011-12-31',\n",
    "           '2012-03-31', '2012-06-30', '2012-09-30', '2012-12-31',\n",
    "           '2013-03-31', '2013-06-30', '2013-09-30', '2013-12-31',\n",
    "           '2014-03-31', '2014-06-30', '2014-09-30', '2014-12-31',\n",
    "           '2015-03-31', '2015-06-30', '2015-09-30', '2015-12-31',\n",
    "           '2016-03-31', '2016-06-30', '2016-09-30', '2016-12-31',\n",
    "           '2017-03-31', '2017-06-30', '2017-09-30', '2017-12-31',\n",
    "           '2018-03-31']\n",
    "\n",
    "    for month in time_range:\n",
    "\n",
    "        REVENUE_df=M2[['END_DATE','TICKER_SYMBOL','REVENUE_commom_income']]\n",
    "        t=REVENUE_df[REVENUE_df.END_DATE==month].sort_values(by=['END_DATE']).dropna()\n",
    "        t.rename(columns={'REVENUE_commom_income':month},inplace=True)\n",
    "        base=pd.merge(base,t[['TICKER_SYMBOL',month]],how='left',on=['TICKER_SYMBOL'])\n",
    "\n",
    "    base.to_csv(file_path+'ticker_REVENUE_update_v3.csv',index=False)\n",
    "    \n",
    "    return \n",
    "\n",
    "def task_624():\n",
    "    '''\n",
    "    Revenue_0624.csv,\n",
    "    '''\n",
    "\n",
    "    submit=pd.read_csv(file_path+'check_revence_v3.csv')\n",
    "    submit=submit[['TICKER_SYMBOL']]\n",
    "    submit.drop_duplicates(inplace=True)\n",
    "    submit\n",
    "\n",
    "    M2=pd.read_csv(mid_path+'table_base.csv')\n",
    "    base=submit\n",
    "    base\n",
    "\n",
    "    time_range=['2009-03-31', '2009-06-30', '2009-09-30', '2009-12-31',\n",
    "           '2010-03-31', '2010-06-30', '2010-09-30', '2010-12-31',\n",
    "           '2011-03-31', '2011-06-30', '2011-09-30', '2011-12-31',\n",
    "           '2012-03-31', '2012-06-30', '2012-09-30', '2012-12-31',\n",
    "           '2013-03-31', '2013-06-30', '2013-09-30', '2013-12-31',\n",
    "           '2014-03-31', '2014-06-30', '2014-09-30', '2014-12-31',\n",
    "           '2015-03-31', '2015-06-30', '2015-09-30', '2015-12-31',\n",
    "           '2016-03-31', '2016-06-30', '2016-09-30', '2016-12-31',\n",
    "           '2017-03-31', '2017-06-30', '2017-09-30', '2017-12-31',\n",
    "           '2018-03-31']\n",
    "\n",
    "    for month in time_range:\n",
    "\n",
    "        REVENUE_df=M2[['END_DATE','TICKER_SYMBOL','REVENUE_commom_income']]\n",
    "        t=REVENUE_df[REVENUE_df.END_DATE==month].sort_values(by=['END_DATE']).dropna()\n",
    "        t.rename(columns={'REVENUE_commom_income':month},inplace=True)\n",
    "        base=pd.merge(base,t[['TICKER_SYMBOL',month]],how='left',on=['TICKER_SYMBOL'])\n",
    "\n",
    "    base.to_csv(file_path+'ticker_REVENUE_update_v3.csv',index=False)\n",
    "\n",
    "    T=pd.read_csv(other_path+'ticker_market.csv',encoding='gbk')\n",
    "\n",
    "    A=pd.read_csv(file_path+'ticker_REVENUE_update_v3.csv')\n",
    "\n",
    "    W=pd.merge(A,T[['TICKER_SYMBOL','secShortName']],how='left',on=['TICKER_SYMBOL'])\n",
    "\n",
    "    a=list(W.columns)\n",
    "    a.reverse()\n",
    "\n",
    "    drop_list=[200053, 200054, 200152, 200160, 200168, 200468, 200512, 200706,\n",
    "           200771, 200986, 200992, 900929, 900939, 900948, 900951, 900953,\n",
    "           900956, 900957,601360]\n",
    "\n",
    "    W[~W.TICKER_SYMBOL.isin(drop_list)].to_csv(file_path+'Revenue_0624.csv',index=False,encoding='gbk')\n",
    "\n",
    "    e=pd.read_csv(file_path+'Revenue_0624.csv',encoding='gbk')\n",
    "    t=e.count(axis=1).reset_index()\n",
    "    t[0]=t[0]-2\n",
    "    pd.concat([e[['TICKER_SYMBOL','secShortName']],t],axis=1).to_csv(file_path+'Revenue_counts_0624.csv',index=False,encoding='gbk')\n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "\n",
    "##----------------------补齐缺失index信息-----------------------------\n",
    "def add_index():\n",
    "\n",
    "    '''\n",
    "    生成index_info_add.csv，再与原index_info_all.csv结合，生成新的index_info文件\n",
    "\n",
    "    '''\n",
    "\n",
    "    files= os.listdir(other_path+'index/')\n",
    "    data = []\n",
    "    S=0\n",
    "    for file in files:  \n",
    "        print(file)\n",
    "        feature = pd.read_excel(other_path+'index/'+file,encoding='gbk')\n",
    "        feature['ticker']=int(file[0:6])\n",
    "        S+=len(feature)-2\n",
    "        data.append(feature)\n",
    "\n",
    "    print(S)\n",
    "    IN=pd.concat(data)\n",
    "\n",
    "    IN.dropna(subset=['交易日期'],how='any',axis=0,inplace=True)\n",
    "    print(len(IN))\n",
    "    IN=IN[IN.交易日期>'2009-01-01']\n",
    "\n",
    "    IN.drop(['序号'],axis=1,inplace=True)\n",
    "\n",
    "    IN.replace('－',np.nan,inplace=True)\n",
    "    IN.rename(columns={'交易日期':'tradeDate','开盘':'openIndex','最高':'highestIndex','最低':'lowestIndex','收盘':'closeIndex',\\\n",
    "                      '涨跌':'CHG','涨跌幅[%]':'CHGPct','成交量[万股]':'turnoverVol','成交额[亿元]':'turnoverValue'},inplace=True)\n",
    "\n",
    "\n",
    "    IN['indexID']=IN.ticker.astype(str)+'.ZICN'\n",
    "    IN['porgFullName']='上海申银万国证券研究所有限公司'\n",
    "    IN['secShortName']='啦啦啦'\n",
    "    IN['exchangeCD']='XSHG'\n",
    "    IN['preCloseIndex']=0\n",
    "    IN=IN[['indexID', 'ticker', 'porgFullName', 'secShortName', 'exchangeCD',\n",
    "           'tradeDate', 'preCloseIndex', 'openIndex', 'lowestIndex',\n",
    "           'highestIndex', 'closeIndex', 'turnoverVol', 'turnoverValue', 'CHG',\n",
    "           'CHGPct']]\n",
    "    IN['tradeDate']=IN['tradeDate'].apply(lambda x:datetime.datetime.strftime(x,'%Y-%m-%d'))\n",
    "\n",
    "    IN['turnoverVol']=IN['turnoverVol']*10000\n",
    "\n",
    "    IN['turnoverValue']=IN['turnoverValue']*(10**8)\n",
    "    IN['CHGPct']=IN['CHGPct']/100\n",
    "\n",
    "\n",
    "    IN.to_csv(file_path+'index_info_add.csv',index=False,encoding='gbk')\n",
    "\n",
    "\n",
    "    ##---处理 index_info_all-------------------\n",
    "\n",
    "    origin=pd.read_csv(file_path+'index_info_all.csv',encoding='gbk')\n",
    "    origin\n",
    "\n",
    "    add=pd.read_csv(file_path+'index_info_add.csv',encoding='gbk')\n",
    "    add\n",
    "\n",
    "\n",
    "    s_list=add.ticker.unique()\n",
    "    s_list\n",
    "\n",
    "    pd.concat([origin[~origin.ticker.isin(s_list)],add]).to_csv(mid_path+'index_info_new_2.csv',index=False,encoding='gbk')\n",
    "    \n",
    "    pass\n",
    "\n",
    "def NewRound(x):\n",
    "    a, b = math.modf(x)\n",
    "    if a >= 0.5 :\n",
    "        return int(b) + 1\n",
    "    else:\n",
    "        return int(b)\n",
    "\n",
    "\n",
    "def get_weight():\n",
    "    '''\n",
    "    开始是采用5.31号市值，后来精确到亿，四舍五入。\n",
    "    \n",
    "    '''\n",
    "    market_value=pd.read_excel(other_path+'[New] Market Data_20180613.xlsx')\n",
    "    market_value=market_value[market_value.END_DATE_=='2018-05-31']\n",
    "    if len(market_value[market_value.MARKET_VALUE<0])!=0:\n",
    "        print('出现负值')\n",
    "    market_value['weight']=market_value.MARKET_VALUE.apply(lambda x:math.log(max(NewRound(x/(10**8)),2),2))\n",
    "    market_value[['TICKER_SYMBOL','weight']].to_csv(other_path+'weight.csv',index=False)\n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "file_path='./files/'\n",
    "\n",
    "\n",
    "def switch(class_by,indus_index,k):\n",
    "    '''\n",
    "    class_by=['classifi_by_14L1','classifi_by_14L2','classifi_by_14L3']\n",
    "    indus_index=['lv1','lv2','lv3']\n",
    "    k=0-829,1888\n",
    "    '''\n",
    "\n",
    "    fea_im=pd.read_excel('Importance.xlsx')\n",
    "    fea_im=fea_im[['feature','CN']]\n",
    "    fea_im['rank']=[x for x in range(1,len(fea_im)+1)]\n",
    "    fea_im['flag']=0\n",
    "    fea_im\n",
    "\n",
    "    #------处理k值-----------------\n",
    "    if k==1888:\n",
    "        fea_im.loc[fea_im[~fea_im.feature.str.contains('classifi_by_')].index,'flag']=1\n",
    "    elif k==0:\n",
    "        pass\n",
    "    else:\n",
    "        dummy_col=[]\n",
    "        undummy_col=[]\n",
    "        i=0\n",
    "        while(len(undummy_col)!=k):\n",
    "            if 'classifi_by_14L' in fea_im.feature[i]:\n",
    "                dummy_col.append(fea_im.feature[i])\n",
    "            else:\n",
    "                undummy_col.append(fea_im.feature[i])\n",
    "            i+=1\n",
    "        undummy_col.extend(dummy_col)\n",
    "        index=fea_im[fea_im.feature.isin(undummy_col)].index\n",
    "        fea_im.loc[index,'flag']=1\n",
    "\n",
    "    #处理开关\n",
    "\n",
    "    # class_by=['classifi_by_14L1']\n",
    "    # indus_index=['lv1']\n",
    "    class_by.extend(indus_index)\n",
    "    col=[]\n",
    "    for item in class_by:\n",
    "        col.extend(fea_im.feature[fea_im.feature.str.contains(item)].values)\n",
    "\n",
    "    index_2=fea_im[fea_im.feature.isin(col)].index\n",
    "    fea_im.loc[index_2,'flag']=1\n",
    "    fea_im[['feature','CN','rank','flag']].to_csv(file_path+'feature_table.csv',index=False,encoding='gbk')\n",
    "\n",
    "    return \n",
    "\n",
    "# k=1888\n",
    "# class_by=['classifi_by_14L1','classifi_by_14L2',]\n",
    "# indus_index=['lv1','lv2','lv3']\n",
    "# switch(class_by,indus_index,k)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
